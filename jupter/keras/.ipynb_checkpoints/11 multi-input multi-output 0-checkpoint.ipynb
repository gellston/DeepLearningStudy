{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러개의 텐서를 입력받아 여러개의 텐서를 출력하는 모델을 알아 봅시다. 그 예로서 <br>\n",
    "yA = a * (xA * xB ) + b <br>\n",
    "yB = c * (xA + xB ) + d <br>\n",
    "의 관계를 가지는 데이터에서 a, b, c, d 를 학습으로 발견해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "a = 2; b = 1; c = 2; d = 1\n",
    "\n",
    "x_train_A = np.random.rand(1000,1) * 2 - 1\n",
    "x_train_B = np.random.rand(1000,1) * 2 - 1\n",
    "y_train_A = a * (x_train_A * x_train_B) + b\n",
    "y_train_B = c * (x_train_A + x_train_B) + d\n",
    "\n",
    "xA = layers.Input((1,), name='xA')\n",
    "xB = layers.Input((1,), name='xB')\n",
    "hA = layers.Multiply(name='mul')([xA, xB])\n",
    "hB = layers.Add(name='add')([xA, xB])\n",
    "yA = layers.Dense(1, name='yA')(hA)\n",
    "yB = layers.Dense(1, name='yB')(hB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 정의할때 여러개의 입력과 출력이 있으면 리스트 형식으로 넣어주면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "xA (InputLayer)                 [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "xB (InputLayer)                 [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mul (Multiply)                  (None, 1)            0           xA[0][0]                         \n",
      "                                                                 xB[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 1)            0           xA[0][0]                         \n",
      "                                                                 xB[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "yA (Dense)                      (None, 1)            2           mul[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "yB (Dense)                      (None, 1)            2           add[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Model([xA, xB], [yA, yB])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 출력에 mse loss 를 적용하고 그것들을 더해 전체 loss 를 정의하려면 다음과 같이 컴파일하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각 출력에 특정 loss 와 가중치를 지정하여 다 더해 전체 loss 를 정의하려면 다음과 같이 컴파일하면 됩니다. 전체 loss 는 하나이므로 그것을 최적화하는 optimizer 는 하나입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss={'yA': 'mse', 'yB': 'mse'},\n",
    "              loss_weights={'yA': 2., 'yB': 1.})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 정의할 때처럼 여러 입력과 출력 데이터를 리스트 형식으로 넣고 학습을 합니다. 전체 loss 는 각각 loss 에 가중치를 곱해서 다 더한 값이라는것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 1s 987us/sample - loss: 8.5042 - yA_loss: 2.0772 - yB_loss: 4.3498 - val_loss: 8.0464 - val_yA_loss: 1.9609 - val_yB_loss: 4.1246\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 321us/sample - loss: 7.6583 - yA_loss: 1.8556 - yB_loss: 3.9471 - val_loss: 7.2661 - val_yA_loss: 1.7598 - val_yB_loss: 3.7466\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 306us/sample - loss: 6.8964 - yA_loss: 1.6609 - yB_loss: 3.5746 - val_loss: 6.5604 - val_yA_loss: 1.5822 - val_yB_loss: 3.3960\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 6.2119 - yA_loss: 1.4890 - yB_loss: 3.2339 - val_loss: 5.9216 - val_yA_loss: 1.4249 - val_yB_loss: 3.0719\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 314us/sample - loss: 5.5964 - yA_loss: 1.3375 - yB_loss: 2.9214 - val_loss: 5.3551 - val_yA_loss: 1.2876 - val_yB_loss: 2.7799\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 5.0467 - yA_loss: 1.2048 - yB_loss: 2.6371 - val_loss: 4.8406 - val_yA_loss: 1.1662 - val_yB_loss: 2.5082\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 4.5509 - yA_loss: 1.0877 - yB_loss: 2.3755 - val_loss: 4.3868 - val_yA_loss: 1.0614 - val_yB_loss: 2.2639\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 4.1069 - yA_loss: 0.9852 - yB_loss: 2.1365 - val_loss: 3.9734 - val_yA_loss: 0.9672 - val_yB_loss: 2.0389\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 300us/sample - loss: 3.7096 - yA_loss: 0.8956 - yB_loss: 1.9184 - val_loss: 3.5963 - val_yA_loss: 0.8837 - val_yB_loss: 1.8289\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 3.3521 - yA_loss: 0.8166 - yB_loss: 1.7189 - val_loss: 3.2671 - val_yA_loss: 0.8119 - val_yB_loss: 1.6432\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 328us/sample - loss: 3.0322 - yA_loss: 0.7471 - yB_loss: 1.5381 - val_loss: 2.9640 - val_yA_loss: 0.7477 - val_yB_loss: 1.4686\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 300us/sample - loss: 2.7438 - yA_loss: 0.6855 - yB_loss: 1.3728 - val_loss: 2.6941 - val_yA_loss: 0.6904 - val_yB_loss: 1.3132\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 332us/sample - loss: 2.4848 - yA_loss: 0.6310 - yB_loss: 1.2229 - val_loss: 2.4497 - val_yA_loss: 0.6387 - val_yB_loss: 1.1723\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 300us/sample - loss: 2.2515 - yA_loss: 0.5821 - yB_loss: 1.0873 - val_loss: 2.2263 - val_yA_loss: 0.5922 - val_yB_loss: 1.0419\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 2.0410 - yA_loss: 0.5383 - yB_loss: 0.9644 - val_loss: 2.0217 - val_yA_loss: 0.5492 - val_yB_loss: 0.9233\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 330us/sample - loss: 1.8496 - yA_loss: 0.4983 - yB_loss: 0.8530 - val_loss: 1.8400 - val_yA_loss: 0.5111 - val_yB_loss: 0.8178\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 345us/sample - loss: 1.6761 - yA_loss: 0.4619 - yB_loss: 0.7523 - val_loss: 1.6686 - val_yA_loss: 0.4741 - val_yB_loss: 0.7204\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 1.5179 - yA_loss: 0.4282 - yB_loss: 0.6616 - val_loss: 1.5143 - val_yA_loss: 0.4409 - val_yB_loss: 0.6326\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 319us/sample - loss: 1.3737 - yA_loss: 0.3970 - yB_loss: 0.5796 - val_loss: 1.3733 - val_yA_loss: 0.4095 - val_yB_loss: 0.5542\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 337us/sample - loss: 1.2418 - yA_loss: 0.3680 - yB_loss: 0.5057 - val_loss: 1.2450 - val_yA_loss: 0.3803 - val_yB_loss: 0.4844\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 335us/sample - loss: 1.1215 - yA_loss: 0.3409 - yB_loss: 0.4397 - val_loss: 1.1241 - val_yA_loss: 0.3524 - val_yB_loss: 0.4193\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 344us/sample - loss: 1.0108 - yA_loss: 0.3153 - yB_loss: 0.3802 - val_loss: 1.0150 - val_yA_loss: 0.3261 - val_yB_loss: 0.3627\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 313us/sample - loss: 0.9100 - yA_loss: 0.2913 - yB_loss: 0.3275 - val_loss: 0.9127 - val_yA_loss: 0.3009 - val_yB_loss: 0.3109\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 349us/sample - loss: 0.8173 - yA_loss: 0.2685 - yB_loss: 0.2803 - val_loss: 0.8224 - val_yA_loss: 0.2780 - val_yB_loss: 0.2663\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 314us/sample - loss: 0.7328 - yA_loss: 0.2471 - yB_loss: 0.2386 - val_loss: 0.7370 - val_yA_loss: 0.2555 - val_yB_loss: 0.2260\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 0.6556 - yA_loss: 0.2269 - yB_loss: 0.2019 - val_loss: 0.6594 - val_yA_loss: 0.2344 - val_yB_loss: 0.1906\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 0.5853 - yA_loss: 0.2079 - yB_loss: 0.1696 - val_loss: 0.5891 - val_yA_loss: 0.2147 - val_yB_loss: 0.1596\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 342us/sample - loss: 0.5215 - yA_loss: 0.1900 - yB_loss: 0.1414 - val_loss: 0.5240 - val_yA_loss: 0.1957 - val_yB_loss: 0.1326\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 0.4635 - yA_loss: 0.1732 - yB_loss: 0.1171 - val_loss: 0.4658 - val_yA_loss: 0.1783 - val_yB_loss: 0.1093\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 319us/sample - loss: 0.4109 - yA_loss: 0.1574 - yB_loss: 0.0961 - val_loss: 0.4133 - val_yA_loss: 0.1619 - val_yB_loss: 0.0894\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 347us/sample - loss: 0.3636 - yA_loss: 0.1427 - yB_loss: 0.0781 - val_loss: 0.3664 - val_yA_loss: 0.1469 - val_yB_loss: 0.0725\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 322us/sample - loss: 0.3210 - yA_loss: 0.1290 - yB_loss: 0.0630 - val_loss: 0.3220 - val_yA_loss: 0.1320 - val_yB_loss: 0.0581\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 0.2826 - yA_loss: 0.1162 - yB_loss: 0.0502 - val_loss: 0.2839 - val_yA_loss: 0.1189 - val_yB_loss: 0.0461\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 341us/sample - loss: 0.2482 - yA_loss: 0.1043 - yB_loss: 0.0397 - val_loss: 0.2493 - val_yA_loss: 0.1066 - val_yB_loss: 0.0362\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 333us/sample - loss: 0.2175 - yA_loss: 0.0933 - yB_loss: 0.0310 - val_loss: 0.2179 - val_yA_loss: 0.0950 - val_yB_loss: 0.0280\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 332us/sample - loss: 0.1900 - yA_loss: 0.0831 - yB_loss: 0.0239 - val_loss: 0.1908 - val_yA_loss: 0.0846 - val_yB_loss: 0.0215\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 0.1657 - yA_loss: 0.0738 - yB_loss: 0.0182 - val_loss: 0.1659 - val_yA_loss: 0.0748 - val_yB_loss: 0.0162\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 0.1439 - yA_loss: 0.0651 - yB_loss: 0.0136 - val_loss: 0.1445 - val_yA_loss: 0.0662 - val_yB_loss: 0.0121\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 328us/sample - loss: 0.1248 - yA_loss: 0.0573 - yB_loss: 0.0101 - val_loss: 0.1249 - val_yA_loss: 0.0580 - val_yB_loss: 0.0089\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 337us/sample - loss: 0.1077 - yA_loss: 0.0502 - yB_loss: 0.0074 - val_loss: 0.1080 - val_yA_loss: 0.0508 - val_yB_loss: 0.0064\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 350us/sample - loss: 0.0928 - yA_loss: 0.0437 - yB_loss: 0.0053 - val_loss: 0.0926 - val_yA_loss: 0.0440 - val_yB_loss: 0.0046\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 339us/sample - loss: 0.0795 - yA_loss: 0.0379 - yB_loss: 0.0037 - val_loss: 0.0792 - val_yA_loss: 0.0380 - val_yB_loss: 0.0032\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 332us/sample - loss: 0.0679 - yA_loss: 0.0327 - yB_loss: 0.0026 - val_loss: 0.0673 - val_yA_loss: 0.0326 - val_yB_loss: 0.0022\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 0.0577 - yA_loss: 0.0280 - yB_loss: 0.0017 - val_loss: 0.0572 - val_yA_loss: 0.0279 - val_yB_loss: 0.0015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 327us/sample - loss: 0.0487 - yA_loss: 0.0238 - yB_loss: 0.0012 - val_loss: 0.0482 - val_yA_loss: 0.0236 - val_yB_loss: 9.6817e-04\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 0.0410 - yA_loss: 0.0201 - yB_loss: 7.5682e-04 - val_loss: 0.0405 - val_yA_loss: 0.0199 - val_yB_loss: 6.2277e-04\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 0.0342 - yA_loss: 0.0169 - yB_loss: 4.8210e-04 - val_loss: 0.0336 - val_yA_loss: 0.0166 - val_yB_loss: 3.9068e-04\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 0.0284 - yA_loss: 0.0140 - yB_loss: 2.9937e-04 - val_loss: 0.0279 - val_yA_loss: 0.0138 - val_yB_loss: 2.3881e-04\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 336us/sample - loss: 0.0234 - yA_loss: 0.0116 - yB_loss: 1.8132e-04 - val_loss: 0.0228 - val_yA_loss: 0.0113 - val_yB_loss: 1.4346e-04\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 0.0191 - yA_loss: 0.0095 - yB_loss: 1.0722e-04 - val_loss: 0.0185 - val_yA_loss: 0.0092 - val_yB_loss: 8.3383e-05\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 0.0155 - yA_loss: 0.0077 - yB_loss: 6.1530e-05 - val_loss: 0.0149 - val_yA_loss: 0.0074 - val_yB_loss: 4.6991e-05\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 0.0124 - yA_loss: 0.0062 - yB_loss: 3.4292e-05 - val_loss: 0.0119 - val_yA_loss: 0.0060 - val_yB_loss: 2.5925e-05\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 0.0099 - yA_loss: 0.0049 - yB_loss: 1.8574e-05 - val_loss: 0.0094 - val_yA_loss: 0.0047 - val_yB_loss: 1.3624e-05\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 0.0078 - yA_loss: 0.0039 - yB_loss: 9.7019e-06 - val_loss: 0.0074 - val_yA_loss: 0.0037 - val_yB_loss: 7.0566e-06\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 0.0060 - yA_loss: 0.0030 - yB_loss: 4.9100e-06 - val_loss: 0.0057 - val_yA_loss: 0.0028 - val_yB_loss: 3.4798e-06\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 0.0046 - yA_loss: 0.0023 - yB_loss: 2.3980e-06 - val_loss: 0.0043 - val_yA_loss: 0.0022 - val_yB_loss: 1.6388e-06\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 0.0035 - yA_loss: 0.0018 - yB_loss: 1.1228e-06 - val_loss: 0.0033 - val_yA_loss: 0.0016 - val_yB_loss: 7.6026e-07\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 290us/sample - loss: 0.0026 - yA_loss: 0.0013 - yB_loss: 5.0583e-07 - val_loss: 0.0024 - val_yA_loss: 0.0012 - val_yB_loss: 3.4217e-07\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 300us/sample - loss: 0.0019 - yA_loss: 9.6553e-04 - yB_loss: 2.1922e-07 - val_loss: 0.0018 - val_yA_loss: 8.8245e-04 - val_yB_loss: 1.4234e-07\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 0.0014 - yA_loss: 6.9931e-04 - yB_loss: 9.0321e-08 - val_loss: 0.0013 - val_yA_loss: 6.3704e-04 - val_yB_loss: 5.6485e-08\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 294us/sample - loss: 9.9863e-04 - yA_loss: 4.9930e-04 - yB_loss: 3.5335e-08 - val_loss: 8.9584e-04 - val_yA_loss: 4.4791e-04 - val_yB_loss: 2.1741e-08\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 6.9993e-04 - yA_loss: 3.4996e-04 - yB_loss: 1.3196e-08 - val_loss: 6.2488e-04 - val_yA_loss: 3.1244e-04 - val_yB_loss: 7.8296e-09\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 4.8175e-04 - yA_loss: 2.4087e-04 - yB_loss: 4.6483e-09 - val_loss: 4.2732e-04 - val_yA_loss: 2.1366e-04 - val_yB_loss: 2.6101e-09\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 3.2547e-04 - yA_loss: 1.6273e-04 - yB_loss: 1.5310e-09 - val_loss: 2.8299e-04 - val_yA_loss: 1.4149e-04 - val_yB_loss: 8.3193e-10\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 2.1516e-04 - yA_loss: 1.0758e-04 - yB_loss: 4.9209e-10 - val_loss: 1.8623e-04 - val_yA_loss: 9.3114e-05 - val_yB_loss: 2.9524e-10\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 344us/sample - loss: 1.3942e-04 - yA_loss: 6.9711e-05 - yB_loss: 1.5154e-10 - val_loss: 1.1906e-04 - val_yA_loss: 5.9531e-05 - val_yB_loss: 5.3596e-11\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 345us/sample - loss: 8.8273e-05 - yA_loss: 4.4137e-05 - yB_loss: 4.2791e-11 - val_loss: 7.3887e-05 - val_yA_loss: 3.6944e-05 - val_yB_loss: 4.3855e-11\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 5.4545e-05 - yA_loss: 2.7272e-05 - yB_loss: 3.8998e-11 - val_loss: 4.5143e-05 - val_yA_loss: 2.2571e-05 - val_yB_loss: 3.8782e-11\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 3.2901e-05 - yA_loss: 1.6450e-05 - yB_loss: 3.6623e-11 - val_loss: 2.6797e-05 - val_yA_loss: 1.3399e-05 - val_yB_loss: 3.7424e-11\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 1.9260e-05 - yA_loss: 9.6302e-06 - yB_loss: 3.6119e-11 - val_loss: 1.5623e-05 - val_yA_loss: 7.8116e-06 - val_yB_loss: 3.6458e-11\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 1.1014e-05 - yA_loss: 5.5072e-06 - yB_loss: 3.1142e-11 - val_loss: 8.7094e-06 - val_yA_loss: 4.3547e-06 - val_yB_loss: 3.1637e-11\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 6.1058e-06 - yA_loss: 3.0529e-06 - yB_loss: 2.6683e-11 - val_loss: 4.7169e-06 - val_yA_loss: 2.3584e-06 - val_yB_loss: 2.3453e-11\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 3.2736e-06 - yA_loss: 1.6368e-06 - yB_loss: 2.2041e-11 - val_loss: 2.5178e-06 - val_yA_loss: 1.2589e-06 - val_yB_loss: 2.2509e-11\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 1.6998e-06 - yA_loss: 8.4989e-07 - yB_loss: 2.1691e-11 - val_loss: 1.2826e-06 - val_yA_loss: 6.4130e-07 - val_yB_loss: 2.2378e-11\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 8.5263e-07 - yA_loss: 4.2631e-07 - yB_loss: 1.8341e-11 - val_loss: 6.2490e-07 - val_yA_loss: 3.1244e-07 - val_yB_loss: 1.7976e-11\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 4.1277e-07 - yA_loss: 2.0638e-07 - yB_loss: 1.7416e-11 - val_loss: 2.9714e-07 - val_yA_loss: 1.4856e-07 - val_yB_loss: 1.7976e-11\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 1.9142e-07 - yA_loss: 9.5704e-08 - yB_loss: 1.6593e-11 - val_loss: 1.3652e-07 - val_yA_loss: 6.8252e-08 - val_yB_loss: 1.4900e-11\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 8.5960e-08 - yA_loss: 4.2973e-08 - yB_loss: 1.4309e-11 - val_loss: 5.7834e-08 - val_yA_loss: 2.8910e-08 - val_yB_loss: 1.4900e-11\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 0s 294us/sample - loss: 3.6560e-08 - yA_loss: 1.8273e-08 - yB_loss: 1.4103e-11 - val_loss: 2.4603e-08 - val_yA_loss: 1.2295e-08 - val_yB_loss: 1.2623e-11\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 1.5060e-08 - yA_loss: 7.5240e-09 - yB_loss: 1.2207e-11 - val_loss: 9.7013e-09 - val_yA_loss: 4.8443e-09 - val_yB_loss: 1.2623e-11\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 5.8448e-09 - yA_loss: 2.9168e-09 - yB_loss: 1.1291e-11 - val_loss: 3.7436e-09 - val_yA_loss: 1.8661e-09 - val_yB_loss: 1.1338e-11\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 2.1755e-09 - yA_loss: 1.0824e-09 - yB_loss: 1.0761e-11 - val_loss: 1.3403e-09 - val_yA_loss: 6.6480e-10 - val_yB_loss: 1.0648e-11\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 294us/sample - loss: 7.7275e-10 - yA_loss: 3.8194e-10 - yB_loss: 8.8649e-12 - val_loss: 4.6585e-10 - val_yA_loss: 2.2883e-10 - val_yB_loss: 8.1937e-12\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 300us/sample - loss: 2.5808e-10 - yA_loss: 1.2527e-10 - yB_loss: 7.5461e-12 - val_loss: 1.6170e-10 - val_yA_loss: 7.7539e-11 - val_yB_loss: 6.6204e-12\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 292us/sample - loss: 9.4263e-11 - yA_loss: 4.3940e-11 - yB_loss: 6.3824e-12 - val_loss: 5.2079e-11 - val_yA_loss: 2.2729e-11 - val_yB_loss: 6.6204e-12\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 290us/sample - loss: 2.6811e-11 - yA_loss: 1.0221e-11 - yB_loss: 6.3703e-12 - val_loss: 2.1294e-11 - val_yA_loss: 7.6095e-12 - val_yB_loss: 6.0748e-12\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 294us/sample - loss: 1.6663e-11 - yA_loss: 5.8117e-12 - yB_loss: 5.0396e-12 - val_loss: 1.4985e-11 - val_yA_loss: 5.9096e-12 - val_yB_loss: 3.1663e-12\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 296us/sample - loss: 1.3899e-11 - yA_loss: 5.4218e-12 - yB_loss: 3.0552e-12 - val_loss: 1.4985e-11 - val_yA_loss: 5.9096e-12 - val_yB_loss: 3.1663e-12\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 1.3899e-11 - yA_loss: 5.4218e-12 - yB_loss: 3.0552e-12 - val_loss: 1.4985e-11 - val_yA_loss: 5.9096e-12 - val_yB_loss: 3.1663e-12\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 1.3105e-11 - yA_loss: 5.0249e-12 - yB_loss: 3.0552e-12 - val_loss: 1.3067e-11 - val_yA_loss: 4.9504e-12 - val_yB_loss: 3.1663e-12\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 288us/sample - loss: 1.1889e-11 - yA_loss: 4.4169e-12 - yB_loss: 3.0552e-12 - val_loss: 1.2004e-11 - val_yA_loss: 4.4190e-12 - val_yB_loss: 3.1663e-12\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 1.1192e-11 - yA_loss: 4.0687e-12 - yB_loss: 3.0552e-12 - val_loss: 1.2004e-11 - val_yA_loss: 4.4190e-12 - val_yB_loss: 3.1663e-12\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 293us/sample - loss: 1.0187e-11 - yA_loss: 3.5658e-12 - yB_loss: 3.0552e-12 - val_loss: 9.4652e-12 - val_yA_loss: 3.1495e-12 - val_yB_loss: 3.1663e-12\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 8.7156e-12 - yA_loss: 2.9141e-12 - yB_loss: 2.8874e-12 - val_loss: 9.1277e-12 - val_yA_loss: 3.1495e-12 - val_yB_loss: 2.8288e-12\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 290us/sample - loss: 7.3437e-12 - yA_loss: 2.6052e-12 - yB_loss: 2.1332e-12 - val_loss: 6.4011e-12 - val_yA_loss: 2.3668e-12 - val_yB_loss: 1.6674e-12\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 294us/sample - loss: 5.9664e-12 - yA_loss: 2.1722e-12 - yB_loss: 1.6220e-12 - val_loss: 6.4011e-12 - val_yA_loss: 2.3668e-12 - val_yB_loss: 1.6674e-12\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 5.9664e-12 - yA_loss: 2.1722e-12 - yB_loss: 1.6220e-12 - val_loss: 6.4011e-12 - val_yA_loss: 2.3668e-12 - val_yB_loss: 1.6674e-12\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 292us/sample - loss: 5.9425e-12 - yA_loss: 2.1603e-12 - yB_loss: 1.6220e-12 - val_loss: 6.1512e-12 - val_yA_loss: 2.2419e-12 - val_yB_loss: 1.6674e-12\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 5.7209e-12 - yA_loss: 2.0495e-12 - yB_loss: 1.6220e-12 - val_loss: 6.1512e-12 - val_yA_loss: 2.2419e-12 - val_yB_loss: 1.6674e-12\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 292us/sample - loss: 4.7585e-12 - yA_loss: 1.7073e-12 - yB_loss: 1.3438e-12 - val_loss: 4.5815e-12 - val_yA_loss: 1.6679e-12 - val_yB_loss: 1.2457e-12\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 4.2519e-12 - yA_loss: 1.5404e-12 - yB_loss: 1.1710e-12 - val_loss: 4.5815e-12 - val_yA_loss: 1.6679e-12 - val_yB_loss: 1.2457e-12\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 300us/sample - loss: 3.6816e-12 - yA_loss: 1.2553e-12 - yB_loss: 1.1710e-12 - val_loss: 3.8217e-12 - val_yA_loss: 1.2880e-12 - val_yB_loss: 1.2457e-12\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 3.4298e-12 - yA_loss: 1.1764e-12 - yB_loss: 1.0769e-12 - val_loss: 3.1852e-12 - val_yA_loss: 1.2731e-12 - val_yB_loss: 6.3901e-13\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 2.9702e-12 - yA_loss: 1.1759e-12 - yB_loss: 6.1840e-13 - val_loss: 3.1852e-12 - val_yA_loss: 1.2731e-12 - val_yB_loss: 6.3901e-13\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 2.6530e-12 - yA_loss: 1.0173e-12 - yB_loss: 6.1840e-13 - val_loss: 2.3771e-12 - val_yA_loss: 8.6904e-13 - val_yB_loss: 6.3901e-13\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 2.2061e-12 - yA_loss: 7.9386e-13 - yB_loss: 6.1840e-13 - val_loss: 2.3771e-12 - val_yA_loss: 8.6904e-13 - val_yB_loss: 6.3901e-13\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 330us/sample - loss: 2.1339e-12 - yA_loss: 7.5776e-13 - yB_loss: 6.1840e-13 - val_loss: 2.2276e-12 - val_yA_loss: 7.9430e-13 - val_yB_loss: 6.3901e-13\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 350us/sample - loss: 1.9690e-12 - yA_loss: 6.7531e-13 - yB_loss: 6.1840e-13 - val_loss: 1.8311e-12 - val_yA_loss: 5.9604e-13 - val_yB_loss: 6.3901e-13\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 338us/sample - loss: 1.6983e-12 - yA_loss: 5.4044e-13 - yB_loss: 6.1740e-13 - val_loss: 1.8316e-12 - val_yA_loss: 5.9604e-13 - val_yB_loss: 6.3949e-13\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 324us/sample - loss: 1.3648e-12 - yA_loss: 5.4044e-13 - yB_loss: 2.8390e-13 - val_loss: 1.4576e-12 - val_yA_loss: 5.9604e-13 - val_yB_loss: 2.6553e-13\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 309us/sample - loss: 1.3280e-12 - yA_loss: 5.4044e-13 - yB_loss: 2.4712e-13 - val_loss: 1.4576e-12 - val_yA_loss: 5.9604e-13 - val_yB_loss: 2.6553e-13\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 352us/sample - loss: 1.1633e-12 - yA_loss: 4.5809e-13 - yB_loss: 2.4712e-13 - val_loss: 1.2162e-12 - val_yA_loss: 4.7535e-13 - val_yB_loss: 2.6553e-13\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 348us/sample - loss: 1.1115e-12 - yA_loss: 4.3218e-13 - yB_loss: 2.4712e-13 - val_loss: 1.2162e-12 - val_yA_loss: 4.7535e-13 - val_yB_loss: 2.6553e-13\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 367us/sample - loss: 1.0669e-12 - yA_loss: 4.0991e-13 - yB_loss: 2.4712e-13 - val_loss: 1.0173e-12 - val_yA_loss: 3.7586e-13 - val_yB_loss: 2.6553e-13\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 9.2567e-13 - yA_loss: 3.3928e-13 - yB_loss: 2.4712e-13 - val_loss: 1.0173e-12 - val_yA_loss: 3.7586e-13 - val_yB_loss: 2.6553e-13\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 9.0434e-13 - yA_loss: 3.2861e-13 - yB_loss: 2.4712e-13 - val_loss: 8.3689e-13 - val_yA_loss: 2.8568e-13 - val_yB_loss: 2.6553e-13\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 7.5331e-13 - yA_loss: 2.5310e-13 - yB_loss: 2.4712e-13 - val_loss: 8.3689e-13 - val_yA_loss: 2.8568e-13 - val_yB_loss: 2.6553e-13\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 7.5331e-13 - yA_loss: 2.5310e-13 - yB_loss: 2.4712e-13 - val_loss: 8.3689e-13 - val_yA_loss: 2.8568e-13 - val_yB_loss: 2.6553e-13\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 7.0820e-13 - yA_loss: 2.4602e-13 - yB_loss: 2.1616e-13 - val_loss: 5.2260e-13 - val_yA_loss: 2.0542e-13 - val_yB_loss: 1.1176e-13\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 332us/sample - loss: 4.6341e-13 - yA_loss: 1.8292e-13 - yB_loss: 9.7569e-14 - val_loss: 5.2260e-13 - val_yA_loss: 2.0542e-13 - val_yB_loss: 1.1176e-13\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 333us/sample - loss: 4.6341e-13 - yA_loss: 1.8292e-13 - yB_loss: 9.7569e-14 - val_loss: 5.2260e-13 - val_yA_loss: 2.0542e-13 - val_yB_loss: 1.1176e-13\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 318us/sample - loss: 4.6341e-13 - yA_loss: 1.8292e-13 - yB_loss: 9.7569e-14 - val_loss: 5.2260e-13 - val_yA_loss: 2.0542e-13 - val_yB_loss: 1.1176e-13\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 4.6341e-13 - yA_loss: 1.8292e-13 - yB_loss: 9.7569e-14 - val_loss: 5.2260e-13 - val_yA_loss: 2.0542e-13 - val_yB_loss: 1.1176e-13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 3.7934e-13 - yA_loss: 1.4088e-13 - yB_loss: 9.7569e-14 - val_loss: 3.2517e-13 - val_yA_loss: 1.0670e-13 - val_yB_loss: 1.1176e-13\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 2.9623e-13 - yA_loss: 9.9333e-14 - yB_loss: 9.7569e-14 - val_loss: 3.2517e-13 - val_yA_loss: 1.0670e-13 - val_yB_loss: 1.1176e-13\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 328us/sample - loss: 2.9623e-13 - yA_loss: 9.9333e-14 - yB_loss: 9.7569e-14 - val_loss: 3.2517e-13 - val_yA_loss: 1.0670e-13 - val_yB_loss: 1.1176e-13\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 2.9623e-13 - yA_loss: 9.9333e-14 - yB_loss: 9.7569e-14 - val_loss: 3.2517e-13 - val_yA_loss: 1.0670e-13 - val_yB_loss: 1.1176e-13\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 292us/sample - loss: 2.9623e-13 - yA_loss: 9.9333e-14 - yB_loss: 9.7569e-14 - val_loss: 3.2517e-13 - val_yA_loss: 1.0670e-13 - val_yB_loss: 1.1176e-13\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 332us/sample - loss: 2.6851e-13 - yA_loss: 8.6868e-14 - yB_loss: 9.4774e-14 - val_loss: 2.1662e-13 - val_yA_loss: 8.5215e-14 - val_yB_loss: 4.6192e-14\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 320us/sample - loss: 1.9519e-13 - yA_loss: 7.6364e-14 - yB_loss: 4.2464e-14 - val_loss: 2.1662e-13 - val_yA_loss: 8.5215e-14 - val_yB_loss: 4.6192e-14\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 1.9519e-13 - yA_loss: 7.6364e-14 - yB_loss: 4.2464e-14 - val_loss: 2.1662e-13 - val_yA_loss: 8.5215e-14 - val_yB_loss: 4.6192e-14\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 318us/sample - loss: 1.9593e-13 - yA_loss: 7.6735e-14 - yB_loss: 4.2464e-14 - val_loss: 2.1662e-13 - val_yA_loss: 8.5215e-14 - val_yB_loss: 4.6192e-14\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 1.3623e-13 - yA_loss: 4.6885e-14 - yB_loss: 4.2464e-14 - val_loss: 1.3887e-13 - val_yA_loss: 4.6339e-14 - val_yB_loss: 4.6192e-14\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 1.1908e-13 - yA_loss: 3.8308e-14 - yB_loss: 4.2464e-14 - val_loss: 1.3887e-13 - val_yA_loss: 4.6339e-14 - val_yB_loss: 4.6192e-14\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 332us/sample - loss: 1.1908e-13 - yA_loss: 3.8308e-14 - yB_loss: 4.2464e-14 - val_loss: 1.3887e-13 - val_yA_loss: 4.6339e-14 - val_yB_loss: 4.6192e-14\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 1.1908e-13 - yA_loss: 3.8308e-14 - yB_loss: 4.2464e-14 - val_loss: 1.3887e-13 - val_yA_loss: 4.6339e-14 - val_yB_loss: 4.6192e-14\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 332us/sample - loss: 1.1908e-13 - yA_loss: 3.8308e-14 - yB_loss: 4.2464e-14 - val_loss: 1.3887e-13 - val_yA_loss: 4.6339e-14 - val_yB_loss: 4.6192e-14\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 317us/sample - loss: 1.1167e-13 - yA_loss: 3.8308e-14 - yB_loss: 3.5056e-14 - val_loss: 1.1972e-13 - val_yA_loss: 4.6339e-14 - val_yB_loss: 2.7042e-14\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 1.0378e-13 - yA_loss: 3.8308e-14 - yB_loss: 2.7160e-14 - val_loss: 1.1972e-13 - val_yA_loss: 4.6339e-14 - val_yB_loss: 2.7042e-14\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 323us/sample - loss: 8.8119e-14 - yA_loss: 3.0480e-14 - yB_loss: 2.7160e-14 - val_loss: 8.4149e-14 - val_yA_loss: 2.8554e-14 - val_yB_loss: 2.7042e-14\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 313us/sample - loss: 7.8016e-14 - yA_loss: 2.5428e-14 - yB_loss: 2.7160e-14 - val_loss: 8.4149e-14 - val_yA_loss: 2.8554e-14 - val_yB_loss: 2.7042e-14\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 7.8016e-14 - yA_loss: 2.5428e-14 - yB_loss: 2.7160e-14 - val_loss: 8.4149e-14 - val_yA_loss: 2.8554e-14 - val_yB_loss: 2.7042e-14\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 6.8604e-14 - yA_loss: 2.2034e-14 - yB_loss: 2.4536e-14 - val_loss: 4.3951e-14 - val_yA_loss: 1.8326e-14 - val_yB_loss: 7.2980e-15\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 324us/sample - loss: 4.1529e-14 - yA_loss: 1.7224e-14 - yB_loss: 7.0800e-15 - val_loss: 4.3951e-14 - val_yA_loss: 1.8326e-14 - val_yB_loss: 7.2980e-15\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 4.1529e-14 - yA_loss: 1.7224e-14 - yB_loss: 7.0800e-15 - val_loss: 4.3951e-14 - val_yA_loss: 1.8326e-14 - val_yB_loss: 7.2980e-15\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 320us/sample - loss: 4.1529e-14 - yA_loss: 1.7224e-14 - yB_loss: 7.0800e-15 - val_loss: 4.3951e-14 - val_yA_loss: 1.8326e-14 - val_yB_loss: 7.2980e-15\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 4.1529e-14 - yA_loss: 1.7224e-14 - yB_loss: 7.0800e-15 - val_loss: 4.3951e-14 - val_yA_loss: 1.8326e-14 - val_yB_loss: 7.2980e-15\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 335us/sample - loss: 3.0648e-14 - yA_loss: 1.1784e-14 - yB_loss: 7.0800e-15 - val_loss: 2.6143e-14 - val_yA_loss: 9.4223e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 327us/sample - loss: 2.4585e-14 - yA_loss: 8.7527e-15 - yB_loss: 7.0800e-15 - val_loss: 2.6143e-14 - val_yA_loss: 9.4223e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 338us/sample - loss: 2.4585e-14 - yA_loss: 8.7527e-15 - yB_loss: 7.0800e-15 - val_loss: 2.6143e-14 - val_yA_loss: 9.4223e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 2.4585e-14 - yA_loss: 8.7527e-15 - yB_loss: 7.0800e-15 - val_loss: 2.6143e-14 - val_yA_loss: 9.4223e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 2.4585e-14 - yA_loss: 8.7527e-15 - yB_loss: 7.0800e-15 - val_loss: 2.6143e-14 - val_yA_loss: 9.4223e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 309us/sample - loss: 2.4860e-14 - yA_loss: 8.7527e-15 - yB_loss: 7.3543e-15 - val_loss: 2.6143e-14 - val_yA_loss: 9.4223e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 2.4585e-14 - yA_loss: 8.7527e-15 - yB_loss: 7.0800e-15 - val_loss: 2.6143e-14 - val_yA_loss: 9.4223e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 309us/sample - loss: 2.4585e-14 - yA_loss: 8.7527e-15 - yB_loss: 7.0800e-15 - val_loss: 2.6143e-14 - val_yA_loss: 9.4223e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 327us/sample - loss: 2.4585e-14 - yA_loss: 8.7527e-15 - yB_loss: 7.0800e-15 - val_loss: 2.6143e-14 - val_yA_loss: 9.4223e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 2.3524e-14 - yA_loss: 8.2220e-15 - yB_loss: 7.0800e-15 - val_loss: 1.9082e-14 - val_yA_loss: 5.8918e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 323us/sample - loss: 1.8765e-14 - yA_loss: 5.8426e-15 - yB_loss: 7.0800e-15 - val_loss: 1.9082e-14 - val_yA_loss: 5.8918e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 1.8734e-14 - yA_loss: 5.8426e-15 - yB_loss: 7.0489e-15 - val_loss: 1.9082e-14 - val_yA_loss: 5.8918e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 1.8796e-14 - yA_loss: 5.8426e-15 - yB_loss: 7.1111e-15 - val_loss: 1.9082e-14 - val_yA_loss: 5.8918e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 0s 337us/sample - loss: 1.8588e-14 - yA_loss: 5.8426e-15 - yB_loss: 6.9025e-15 - val_loss: 1.9082e-14 - val_yA_loss: 5.8918e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 344us/sample - loss: 1.8956e-14 - yA_loss: 5.8426e-15 - yB_loss: 7.2710e-15 - val_loss: 1.9082e-14 - val_yA_loss: 5.8918e-15 - val_yB_loss: 7.2980e-15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 325us/sample - loss: 1.4433e-14 - yA_loss: 4.0896e-15 - yB_loss: 6.2540e-15 - val_loss: 1.1807e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 1.1542e-14 - yA_loss: 2.3181e-15 - yB_loss: 6.9057e-15 - val_loss: 1.1807e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 349us/sample - loss: 1.2062e-14 - yA_loss: 2.3181e-15 - yB_loss: 7.4259e-15 - val_loss: 1.1807e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 1.1656e-14 - yA_loss: 2.3181e-15 - yB_loss: 7.0201e-15 - val_loss: 1.1807e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 341us/sample - loss: 1.1937e-14 - yA_loss: 2.3536e-15 - yB_loss: 7.2299e-15 - val_loss: 2.1000e-14 - val_yA_loss: 6.8510e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 333us/sample - loss: 1.3115e-14 - yA_loss: 2.4697e-15 - yB_loss: 8.1758e-15 - val_loss: 1.1807e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 1.1563e-14 - yA_loss: 2.3181e-15 - yB_loss: 6.9268e-15 - val_loss: 1.1807e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 1.7556e-14 - yA_loss: 2.3181e-15 - yB_loss: 1.2920e-14 - val_loss: 1.4280e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 9.7710e-15\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 294us/sample - loss: 1.2064e-14 - yA_loss: 2.3181e-15 - yB_loss: 7.4275e-15 - val_loss: 1.1807e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 334us/sample - loss: 1.7391e-14 - yA_loss: 2.3181e-15 - yB_loss: 1.2755e-14 - val_loss: 1.4280e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 9.7710e-15\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 1.5172e-14 - yA_loss: 2.4808e-15 - yB_loss: 1.0210e-14 - val_loss: 1.1807e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 342us/sample - loss: 1.2094e-14 - yA_loss: 2.3181e-15 - yB_loss: 7.4575e-15 - val_loss: 2.5244e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 2.0735e-14\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 313us/sample - loss: 1.4970e-14 - yA_loss: 2.5035e-15 - yB_loss: 9.9630e-15 - val_loss: 1.1807e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 332us/sample - loss: 1.2242e-14 - yA_loss: 2.5135e-15 - yB_loss: 7.2153e-15 - val_loss: 1.1807e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 1.4203e-14 - yA_loss: 2.5724e-15 - yB_loss: 9.0585e-15 - val_loss: 2.5513e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 2.1004e-14\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 1.6842e-14 - yA_loss: 2.4513e-15 - yB_loss: 1.1939e-14 - val_loss: 1.1807e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 1.4805e-14 - yA_loss: 2.9397e-15 - yB_loss: 8.9254e-15 - val_loss: 3.8968e-14 - val_yA_loss: 1.5835e-14 - val_yB_loss: 7.2980e-15\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 313us/sample - loss: 3.2688e-14 - yA_loss: 9.7816e-15 - yB_loss: 1.3125e-14 - val_loss: 1.1807e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 300us/sample - loss: 1.9586e-14 - yA_loss: 2.4375e-15 - yB_loss: 1.4711e-14 - val_loss: 1.1807e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 338us/sample - loss: 1.5182e-14 - yA_loss: 2.9206e-15 - yB_loss: 9.3404e-15 - val_loss: 1.1807e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 324us/sample - loss: 3.2476e-14 - yA_loss: 4.7533e-15 - yB_loss: 2.2970e-14 - val_loss: 1.2040e-13 - val_yA_loss: 2.2547e-15 - val_yB_loss: 1.1589e-13\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 333us/sample - loss: 2.5391e-14 - yA_loss: 5.4044e-15 - yB_loss: 1.4582e-14 - val_loss: 2.5513e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 2.1004e-14\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 314us/sample - loss: 2.9045e-14 - yA_loss: 4.6314e-15 - yB_loss: 1.9783e-14 - val_loss: 4.1441e-14 - val_yA_loss: 1.5835e-14 - val_yB_loss: 9.7710e-15\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 342us/sample - loss: 2.4332e-14 - yA_loss: 6.7272e-15 - yB_loss: 1.0878e-14 - val_loss: 1.4280e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 9.7710e-15\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 335us/sample - loss: 3.7513e-14 - yA_loss: 7.4809e-15 - yB_loss: 2.2552e-14 - val_loss: 1.1807e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 2.6383e-14 - yA_loss: 4.7895e-15 - yB_loss: 1.6804e-14 - val_loss: 1.1807e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 7.2980e-15\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 333us/sample - loss: 3.1613e-14 - yA_loss: 4.1880e-15 - yB_loss: 2.3237e-14 - val_loss: 2.5513e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 2.1004e-14\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 5.2642e-14 - yA_loss: 8.2277e-15 - yB_loss: 3.6186e-14 - val_loss: 1.8905e-13 - val_yA_loss: 2.2547e-15 - val_yB_loss: 1.8454e-13\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 328us/sample - loss: 2.7267e-14 - yA_loss: 3.0092e-15 - yB_loss: 2.1249e-14 - val_loss: 1.4280e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 9.7710e-15\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 328us/sample - loss: 5.5450e-14 - yA_loss: 1.2959e-14 - yB_loss: 2.9532e-14 - val_loss: 2.1973e-13 - val_yA_loss: 1.5835e-14 - val_yB_loss: 1.8806e-13\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 309us/sample - loss: 7.2390e-14 - yA_loss: 2.3059e-14 - yB_loss: 2.6272e-14 - val_loss: 5.4059e-14 - val_yA_loss: 1.6528e-14 - val_yB_loss: 2.1004e-14\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 330us/sample - loss: 9.4156e-14 - yA_loss: 1.1248e-14 - yB_loss: 7.1660e-14 - val_loss: 2.5244e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 2.0735e-14\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 6.2546e-14 - yA_loss: 1.3328e-14 - yB_loss: 3.5891e-14 - val_loss: 4.9084e-14 - val_yA_loss: 2.0893e-14 - val_yB_loss: 7.2980e-15\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 328us/sample - loss: 4.4779e-14 - yA_loss: 7.8141e-15 - yB_loss: 2.9151e-14 - val_loss: 5.3073e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 4.8563e-14\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 1.3903e-13 - yA_loss: 3.5740e-14 - yB_loss: 6.7554e-14 - val_loss: 8.9735e-14 - val_yA_loss: 2.3136e-14 - val_yB_loss: 4.3463e-14\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 333us/sample - loss: 1.1188e-13 - yA_loss: 2.7022e-14 - yB_loss: 5.7840e-14 - val_loss: 3.0097e-13 - val_yA_loss: 6.6710e-14 - val_yB_loss: 1.6755e-13\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 348us/sample - loss: 1.1811e-13 - yA_loss: 1.9627e-14 - yB_loss: 7.8860e-14 - val_loss: 1.4280e-14 - val_yA_loss: 2.2547e-15 - val_yB_loss: 9.7710e-15\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 5.2216e-13 - yA_loss: 1.6761e-14 - yB_loss: 4.8864e-13 - val_loss: 4.7272e-12 - val_yA_loss: 2.2547e-15 - val_yB_loss: 4.7227e-12\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yA_weights\n",
      "[array([[2.]], dtype=float32), array([1.], dtype=float32)]\n",
      "yB_weights\n",
      "[array([[1.9999985]], dtype=float32), array([1.0000017], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([x_train_A, x_train_B], [y_train_A, y_train_B],\n",
    "                 batch_size=8, epochs=200, validation_split=0.2)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc=0)\n",
    "plt.show()\n",
    "\n",
    "print('yA_weights'); print(model.get_layer('yA').get_weights())\n",
    "print('yB_weights'); print(model.get_layer('yB').get_weights())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
