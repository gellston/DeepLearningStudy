{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yA = a * xA + b <br>\n",
    "yB = c * exp(yA) + d * xB + e <br>\n",
    "를 만족하는 데이터에서 a, b, c, d, e 를 발견합시다. 초기 웨이트에 따라 학습이 잘 않될 수도 있으니 여러번 실행해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "xA (InputLayer)                 [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "yA (Dense)                      (None, 1)            2           xA[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "exp (Lambda)                    (None, 1)            0           yA[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "xB (InputLayer)                 [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 2)            0           exp[0][0]                        \n",
      "                                                                 xB[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "yB (Dense)                      (None, 1)            3           concat[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 250.6608 - yA_loss: 4.6008 - yB_loss: 246.0600 - val_loss: 211.1730 - val_yA_loss: 4.1605 - val_yB_loss: 207.0125\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 246.8111 - yA_loss: 4.5033 - yB_loss: 242.3078 - val_loss: 207.7110 - val_yA_loss: 4.0069 - val_yB_loss: 203.7040\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 347us/sample - loss: 243.0605 - yA_loss: 4.2495 - yB_loss: 238.8110 - val_loss: 204.1972 - val_yA_loss: 3.6961 - val_yB_loss: 200.5011\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 338us/sample - loss: 239.0429 - yA_loss: 3.8444 - yB_loss: 235.1984 - val_loss: 200.2263 - val_yA_loss: 3.2755 - val_yB_loss: 196.9508\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 309us/sample - loss: 234.3622 - yA_loss: 3.3505 - yB_loss: 231.0117 - val_loss: 195.5006 - val_yA_loss: 2.8016 - val_yB_loss: 192.6990\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 343us/sample - loss: 228.5602 - yA_loss: 2.8284 - yB_loss: 225.7318 - val_loss: 189.4743 - val_yA_loss: 2.3217 - val_yB_loss: 187.1526\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 344us/sample - loss: 220.8844 - yA_loss: 2.3206 - yB_loss: 218.5637 - val_loss: 181.1821 - val_yA_loss: 1.8652 - val_yB_loss: 179.3169\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 301us/sample - loss: 209.8886 - yA_loss: 1.8440 - yB_loss: 208.0446 - val_loss: 169.4560 - val_yA_loss: 1.4833 - val_yB_loss: 167.9726\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 193.6320 - yA_loss: 1.4568 - yB_loss: 192.1752 - val_loss: 151.9192 - val_yA_loss: 1.1910 - val_yB_loss: 150.7282\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 344us/sample - loss: 169.8428 - yA_loss: 1.1894 - yB_loss: 168.6534 - val_loss: 127.6657 - val_yA_loss: 1.0232 - val_yB_loss: 126.6426\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 317us/sample - loss: 137.0745 - yA_loss: 1.0419 - yB_loss: 136.0326 - val_loss: 96.8084 - val_yA_loss: 0.9725 - val_yB_loss: 95.8359\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 98.4469 - yA_loss: 1.0219 - yB_loss: 97.4250 - val_loss: 64.7687 - val_yA_loss: 1.0079 - val_yB_loss: 63.7608\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 314us/sample - loss: 61.5315 - yA_loss: 1.0577 - yB_loss: 60.4737 - val_loss: 40.6177 - val_yA_loss: 1.0809 - val_yB_loss: 39.5368\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 360us/sample - loss: 37.1829 - yA_loss: 1.1169 - yB_loss: 36.0660 - val_loss: 28.8727 - val_yA_loss: 1.1295 - val_yB_loss: 27.7432\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 343us/sample - loss: 26.2517 - yA_loss: 1.1268 - yB_loss: 25.1249 - val_loss: 24.4262 - val_yA_loss: 1.1159 - val_yB_loss: 23.3103\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 309us/sample - loss: 21.5761 - yA_loss: 1.0904 - yB_loss: 20.4857 - val_loss: 21.7064 - val_yA_loss: 1.0740 - val_yB_loss: 20.6323\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 18.7361 - yA_loss: 1.0394 - yB_loss: 17.6968 - val_loss: 19.2492 - val_yA_loss: 1.0160 - val_yB_loss: 18.2331\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 16.4633 - yA_loss: 0.9847 - yB_loss: 15.4786 - val_loss: 17.0699 - val_yA_loss: 0.9568 - val_yB_loss: 16.1131\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 14.5544 - yA_loss: 0.9245 - yB_loss: 13.6299 - val_loss: 15.2558 - val_yA_loss: 0.9075 - val_yB_loss: 14.3483\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 322us/sample - loss: 12.9221 - yA_loss: 0.8778 - yB_loss: 12.0443 - val_loss: 13.6831 - val_yA_loss: 0.8619 - val_yB_loss: 12.8213\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 11.5429 - yA_loss: 0.8352 - yB_loss: 10.7077 - val_loss: 12.2718 - val_yA_loss: 0.8183 - val_yB_loss: 11.4535\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 10.3680 - yA_loss: 0.7910 - yB_loss: 9.5769 - val_loss: 11.1063 - val_yA_loss: 0.7832 - val_yB_loss: 10.3232\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 320us/sample - loss: 9.3104 - yA_loss: 0.7551 - yB_loss: 8.5554 - val_loss: 10.0026 - val_yA_loss: 0.7458 - val_yB_loss: 9.2568\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 337us/sample - loss: 8.4110 - yA_loss: 0.7223 - yB_loss: 7.6887 - val_loss: 9.0170 - val_yA_loss: 0.7103 - val_yB_loss: 8.3067\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 7.6169 - yA_loss: 0.6897 - yB_loss: 6.9272 - val_loss: 8.2049 - val_yA_loss: 0.6819 - val_yB_loss: 7.5231\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 339us/sample - loss: 6.9239 - yA_loss: 0.6613 - yB_loss: 6.2627 - val_loss: 7.4585 - val_yA_loss: 0.6545 - val_yB_loss: 6.8040\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 328us/sample - loss: 6.3154 - yA_loss: 0.6351 - yB_loss: 5.6803 - val_loss: 6.7930 - val_yA_loss: 0.6286 - val_yB_loss: 6.1645\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 5.7712 - yA_loss: 0.6102 - yB_loss: 5.1610 - val_loss: 6.2172 - val_yA_loss: 0.6062 - val_yB_loss: 5.6110\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 5.3117 - yA_loss: 0.5874 - yB_loss: 4.7244 - val_loss: 5.6640 - val_yA_loss: 0.5806 - val_yB_loss: 5.0835\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 348us/sample - loss: 4.8702 - yA_loss: 0.5700 - yB_loss: 4.3001 - val_loss: 5.1904 - val_yA_loss: 0.5602 - val_yB_loss: 4.6302\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 330us/sample - loss: 4.4850 - yA_loss: 0.5455 - yB_loss: 3.9395 - val_loss: 4.7666 - val_yA_loss: 0.5425 - val_yB_loss: 4.2241\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 4.1440 - yA_loss: 0.5287 - yB_loss: 3.6153 - val_loss: 4.4013 - val_yA_loss: 0.5283 - val_yB_loss: 3.8729\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 3.8375 - yA_loss: 0.5137 - yB_loss: 3.3238 - val_loss: 4.0487 - val_yA_loss: 0.5106 - val_yB_loss: 3.5381\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 330us/sample - loss: 3.5670 - yA_loss: 0.4969 - yB_loss: 3.0701 - val_loss: 3.7447 - val_yA_loss: 0.4968 - val_yB_loss: 3.2479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 3.3134 - yA_loss: 0.4857 - yB_loss: 2.8277 - val_loss: 3.4453 - val_yA_loss: 0.4788 - val_yB_loss: 2.9665\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 3.0877 - yA_loss: 0.4691 - yB_loss: 2.6186 - val_loss: 3.1933 - val_yA_loss: 0.4670 - val_yB_loss: 2.7263\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 292us/sample - loss: 2.8791 - yA_loss: 0.4568 - yB_loss: 2.4224 - val_loss: 2.9611 - val_yA_loss: 0.4548 - val_yB_loss: 2.5063\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 2.6958 - yA_loss: 0.4461 - yB_loss: 2.2497 - val_loss: 2.7544 - val_yA_loss: 0.4432 - val_yB_loss: 2.3112\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 2.5275 - yA_loss: 0.4344 - yB_loss: 2.0931 - val_loss: 2.5631 - val_yA_loss: 0.4341 - val_yB_loss: 2.1291\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 2.3673 - yA_loss: 0.4239 - yB_loss: 1.9434 - val_loss: 2.3840 - val_yA_loss: 0.4214 - val_yB_loss: 1.9626\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 2.2217 - yA_loss: 0.4147 - yB_loss: 1.8070 - val_loss: 2.2256 - val_yA_loss: 0.4129 - val_yB_loss: 1.8127\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 2.0884 - yA_loss: 0.4054 - yB_loss: 1.6829 - val_loss: 2.0789 - val_yA_loss: 0.4028 - val_yB_loss: 1.6761\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 300us/sample - loss: 1.9660 - yA_loss: 0.3956 - yB_loss: 1.5704 - val_loss: 1.9433 - val_yA_loss: 0.3958 - val_yB_loss: 1.5475\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 301us/sample - loss: 1.8579 - yA_loss: 0.3898 - yB_loss: 1.4682 - val_loss: 1.8199 - val_yA_loss: 0.3853 - val_yB_loss: 1.4345\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 1.7456 - yA_loss: 0.3806 - yB_loss: 1.3651 - val_loss: 1.7055 - val_yA_loss: 0.3777 - val_yB_loss: 1.3277\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 1.6425 - yA_loss: 0.3741 - yB_loss: 1.2685 - val_loss: 1.6020 - val_yA_loss: 0.3704 - val_yB_loss: 1.2316\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 1.5514 - yA_loss: 0.3658 - yB_loss: 1.1856 - val_loss: 1.4986 - val_yA_loss: 0.3661 - val_yB_loss: 1.1325\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 294us/sample - loss: 1.4642 - yA_loss: 0.3594 - yB_loss: 1.1047 - val_loss: 1.4048 - val_yA_loss: 0.3586 - val_yB_loss: 1.0462\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 1.3849 - yA_loss: 0.3520 - yB_loss: 1.0329 - val_loss: 1.3242 - val_yA_loss: 0.3550 - val_yB_loss: 0.9692\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 309us/sample - loss: 1.3056 - yA_loss: 0.3476 - yB_loss: 0.9580 - val_loss: 1.2399 - val_yA_loss: 0.3465 - val_yB_loss: 0.8935\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 1.2277 - yA_loss: 0.3416 - yB_loss: 0.8861 - val_loss: 1.1671 - val_yA_loss: 0.3405 - val_yB_loss: 0.8266\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 294us/sample - loss: 1.1579 - yA_loss: 0.3356 - yB_loss: 0.8222 - val_loss: 1.0983 - val_yA_loss: 0.3339 - val_yB_loss: 0.7644\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 1.0951 - yA_loss: 0.3293 - yB_loss: 0.7658 - val_loss: 1.0361 - val_yA_loss: 0.3265 - val_yB_loss: 0.7096\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 1.0248 - yA_loss: 0.3244 - yB_loss: 0.7004 - val_loss: 0.9678 - val_yA_loss: 0.3229 - val_yB_loss: 0.6449\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 0.9673 - yA_loss: 0.3186 - yB_loss: 0.6488 - val_loss: 0.9098 - val_yA_loss: 0.3187 - val_yB_loss: 0.5912\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 0.9090 - yA_loss: 0.3133 - yB_loss: 0.5956 - val_loss: 0.8577 - val_yA_loss: 0.3112 - val_yB_loss: 0.5465\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 0.8578 - yA_loss: 0.3080 - yB_loss: 0.5498 - val_loss: 0.8033 - val_yA_loss: 0.3074 - val_yB_loss: 0.4959\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 0.8042 - yA_loss: 0.3030 - yB_loss: 0.5012 - val_loss: 0.7575 - val_yA_loss: 0.3009 - val_yB_loss: 0.4566\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 0.7572 - yA_loss: 0.2973 - yB_loss: 0.4599 - val_loss: 0.7099 - val_yA_loss: 0.2977 - val_yB_loss: 0.4122\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 318us/sample - loss: 0.7108 - yA_loss: 0.2925 - yB_loss: 0.4183 - val_loss: 0.6664 - val_yA_loss: 0.2923 - val_yB_loss: 0.3741\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 0.6681 - yA_loss: 0.2878 - yB_loss: 0.3803 - val_loss: 0.6271 - val_yA_loss: 0.2856 - val_yB_loss: 0.3415\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 0.6265 - yA_loss: 0.2824 - yB_loss: 0.3441 - val_loss: 0.5924 - val_yA_loss: 0.2794 - val_yB_loss: 0.3131\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 0.5887 - yA_loss: 0.2760 - yB_loss: 0.3128 - val_loss: 0.5561 - val_yA_loss: 0.2775 - val_yB_loss: 0.2786\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 0.5517 - yA_loss: 0.2723 - yB_loss: 0.2794 - val_loss: 0.5260 - val_yA_loss: 0.2683 - val_yB_loss: 0.2577\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 0.5171 - yA_loss: 0.2656 - yB_loss: 0.2515 - val_loss: 0.4898 - val_yA_loss: 0.2662 - val_yB_loss: 0.2236\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 0.4862 - yA_loss: 0.2611 - yB_loss: 0.2251 - val_loss: 0.4603 - val_yA_loss: 0.2586 - val_yB_loss: 0.2018\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 0.4557 - yA_loss: 0.2555 - yB_loss: 0.2002 - val_loss: 0.4319 - val_yA_loss: 0.2542 - val_yB_loss: 0.1777\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 0.4286 - yA_loss: 0.2501 - yB_loss: 0.1786 - val_loss: 0.4061 - val_yA_loss: 0.2482 - val_yB_loss: 0.1579\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 0.4028 - yA_loss: 0.2442 - yB_loss: 0.1586 - val_loss: 0.3819 - val_yA_loss: 0.2427 - val_yB_loss: 0.1393\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 0.3778 - yA_loss: 0.2384 - yB_loss: 0.1394 - val_loss: 0.3596 - val_yA_loss: 0.2374 - val_yB_loss: 0.1222\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 0.3553 - yA_loss: 0.2329 - yB_loss: 0.1224 - val_loss: 0.3387 - val_yA_loss: 0.2310 - val_yB_loss: 0.1077\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 0.3340 - yA_loss: 0.2271 - yB_loss: 0.1069 - val_loss: 0.3187 - val_yA_loss: 0.2247 - val_yB_loss: 0.0940\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 0.3147 - yA_loss: 0.2210 - yB_loss: 0.0937 - val_loss: 0.3006 - val_yA_loss: 0.2195 - val_yB_loss: 0.0811\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 0.2958 - yA_loss: 0.2150 - yB_loss: 0.0807 - val_loss: 0.2837 - val_yA_loss: 0.2124 - val_yB_loss: 0.0713\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 300us/sample - loss: 0.2787 - yA_loss: 0.2090 - yB_loss: 0.0697 - val_loss: 0.2673 - val_yA_loss: 0.2068 - val_yB_loss: 0.0605\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 330us/sample - loss: 0.2631 - yA_loss: 0.2029 - yB_loss: 0.0602 - val_loss: 0.2523 - val_yA_loss: 0.2005 - val_yB_loss: 0.0518\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 0.2477 - yA_loss: 0.1968 - yB_loss: 0.0509 - val_loss: 0.2387 - val_yA_loss: 0.1939 - val_yB_loss: 0.0448\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 0.2339 - yA_loss: 0.1906 - yB_loss: 0.0433 - val_loss: 0.2256 - val_yA_loss: 0.1877 - val_yB_loss: 0.0379\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 300us/sample - loss: 0.2213 - yA_loss: 0.1843 - yB_loss: 0.0369 - val_loss: 0.2134 - val_yA_loss: 0.1813 - val_yB_loss: 0.0321\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 0.2089 - yA_loss: 0.1779 - yB_loss: 0.0309 - val_loss: 0.2020 - val_yA_loss: 0.1758 - val_yB_loss: 0.0262\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 0.1974 - yA_loss: 0.1717 - yB_loss: 0.0257 - val_loss: 0.1910 - val_yA_loss: 0.1692 - val_yB_loss: 0.0218\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 0.1867 - yA_loss: 0.1655 - yB_loss: 0.0212 - val_loss: 0.1812 - val_yA_loss: 0.1632 - val_yB_loss: 0.0180\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 0.1768 - yA_loss: 0.1593 - yB_loss: 0.0176 - val_loss: 0.1714 - val_yA_loss: 0.1561 - val_yB_loss: 0.0152\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 0.1673 - yA_loss: 0.1528 - yB_loss: 0.0145 - val_loss: 0.1621 - val_yA_loss: 0.1501 - val_yB_loss: 0.0120\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 0.1583 - yA_loss: 0.1466 - yB_loss: 0.0118 - val_loss: 0.1535 - val_yA_loss: 0.1438 - val_yB_loss: 0.0097\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 294us/sample - loss: 0.1498 - yA_loss: 0.1404 - yB_loss: 0.0094 - val_loss: 0.1453 - val_yA_loss: 0.1375 - val_yB_loss: 0.0078\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 0.1417 - yA_loss: 0.1342 - yB_loss: 0.0075 - val_loss: 0.1374 - val_yA_loss: 0.1314 - val_yB_loss: 0.0060\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 0.1340 - yA_loss: 0.1280 - yB_loss: 0.0060 - val_loss: 0.1300 - val_yA_loss: 0.1253 - val_yB_loss: 0.0046\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 294us/sample - loss: 0.1265 - yA_loss: 0.1219 - yB_loss: 0.0046 - val_loss: 0.1227 - val_yA_loss: 0.1191 - val_yB_loss: 0.0037\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 0.1195 - yA_loss: 0.1159 - yB_loss: 0.0036 - val_loss: 0.1159 - val_yA_loss: 0.1130 - val_yB_loss: 0.0029\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 0.1128 - yA_loss: 0.1101 - yB_loss: 0.0028 - val_loss: 0.1093 - val_yA_loss: 0.1073 - val_yB_loss: 0.0020\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 0.1063 - yA_loss: 0.1043 - yB_loss: 0.0020 - val_loss: 0.1030 - val_yA_loss: 0.1015 - val_yB_loss: 0.0016\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 0.1001 - yA_loss: 0.0986 - yB_loss: 0.0016 - val_loss: 0.0970 - val_yA_loss: 0.0959 - val_yB_loss: 0.0011\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 0.0942 - yA_loss: 0.0930 - yB_loss: 0.0012 - val_loss: 0.0913 - val_yA_loss: 0.0902 - val_yB_loss: 0.0010\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 0.0886 - yA_loss: 0.0877 - yB_loss: 9.2371e-04 - val_loss: 0.0857 - val_yA_loss: 0.0849 - val_yB_loss: 8.3634e-04\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 0.0832 - yA_loss: 0.0824 - yB_loss: 7.7992e-04 - val_loss: 0.0805 - val_yA_loss: 0.0797 - val_yB_loss: 8.1476e-04\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 0.0780 - yA_loss: 0.0773 - yB_loss: 7.4609e-04 - val_loss: 0.0755 - val_yA_loss: 0.0746 - val_yB_loss: 8.2049e-04\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 0.0731 - yA_loss: 0.0724 - yB_loss: 7.3629e-04 - val_loss: 0.0707 - val_yA_loss: 0.0698 - val_yB_loss: 8.6815e-04\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 0.0685 - yA_loss: 0.0676 - yB_loss: 8.4118e-04 - val_loss: 0.0662 - val_yA_loss: 0.0651 - val_yB_loss: 0.0011\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 0.0640 - yA_loss: 0.0631 - yB_loss: 9.2915e-04 - val_loss: 0.0618 - val_yA_loss: 0.0608 - val_yB_loss: 0.0010\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 318us/sample - loss: 0.0598 - yA_loss: 0.0588 - yB_loss: 0.0010 - val_loss: 0.0577 - val_yA_loss: 0.0564 - val_yB_loss: 0.0013\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 300us/sample - loss: 0.0558 - yA_loss: 0.0545 - yB_loss: 0.0013 - val_loss: 0.0539 - val_yA_loss: 0.0523 - val_yB_loss: 0.0016\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 0.0521 - yA_loss: 0.0506 - yB_loss: 0.0015 - val_loss: 0.0501 - val_yA_loss: 0.0485 - val_yB_loss: 0.0017\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 0.0485 - yA_loss: 0.0469 - yB_loss: 0.0016 - val_loss: 0.0466 - val_yA_loss: 0.0449 - val_yB_loss: 0.0017\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 0.0451 - yA_loss: 0.0433 - yB_loss: 0.0018 - val_loss: 0.0435 - val_yA_loss: 0.0412 - val_yB_loss: 0.0022\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 0.0419 - yA_loss: 0.0400 - yB_loss: 0.0019 - val_loss: 0.0403 - val_yA_loss: 0.0384 - val_yB_loss: 0.0019\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 309us/sample - loss: 0.0389 - yA_loss: 0.0367 - yB_loss: 0.0022 - val_loss: 0.0374 - val_yA_loss: 0.0350 - val_yB_loss: 0.0024\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 0.0361 - yA_loss: 0.0339 - yB_loss: 0.0022 - val_loss: 0.0346 - val_yA_loss: 0.0322 - val_yB_loss: 0.0024\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 0.0334 - yA_loss: 0.0311 - yB_loss: 0.0023 - val_loss: 0.0319 - val_yA_loss: 0.0296 - val_yB_loss: 0.0023\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 0.0308 - yA_loss: 0.0285 - yB_loss: 0.0023 - val_loss: 0.0295 - val_yA_loss: 0.0271 - val_yB_loss: 0.0024\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 300us/sample - loss: 0.0287 - yA_loss: 0.0261 - yB_loss: 0.0026 - val_loss: 0.0272 - val_yA_loss: 0.0247 - val_yB_loss: 0.0025\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 319us/sample - loss: 0.0262 - yA_loss: 0.0238 - yB_loss: 0.0024 - val_loss: 0.0250 - val_yA_loss: 0.0226 - val_yB_loss: 0.0024\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 0.0242 - yA_loss: 0.0218 - yB_loss: 0.0024 - val_loss: 0.0229 - val_yA_loss: 0.0207 - val_yB_loss: 0.0023\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 0.0221 - yA_loss: 0.0198 - yB_loss: 0.0023 - val_loss: 0.0210 - val_yA_loss: 0.0189 - val_yB_loss: 0.0022\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 293us/sample - loss: 0.0202 - yA_loss: 0.0181 - yB_loss: 0.0022 - val_loss: 0.0193 - val_yA_loss: 0.0170 - val_yB_loss: 0.0022\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 0.0187 - yA_loss: 0.0164 - yB_loss: 0.0024 - val_loss: 0.0178 - val_yA_loss: 0.0154 - val_yB_loss: 0.0024\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 0.0169 - yA_loss: 0.0148 - yB_loss: 0.0020 - val_loss: 0.0160 - val_yA_loss: 0.0140 - val_yB_loss: 0.0020\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 0.0154 - yA_loss: 0.0135 - yB_loss: 0.0020 - val_loss: 0.0145 - val_yA_loss: 0.0126 - val_yB_loss: 0.0019\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 0.0141 - yA_loss: 0.0121 - yB_loss: 0.0020 - val_loss: 0.0134 - val_yA_loss: 0.0113 - val_yB_loss: 0.0021\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 0.0128 - yA_loss: 0.0109 - yB_loss: 0.0018 - val_loss: 0.0119 - val_yA_loss: 0.0103 - val_yB_loss: 0.0017\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 0.0117 - yA_loss: 0.0099 - yB_loss: 0.0018 - val_loss: 0.0108 - val_yA_loss: 0.0092 - val_yB_loss: 0.0015\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 300us/sample - loss: 0.0103 - yA_loss: 0.0088 - yB_loss: 0.0015 - val_loss: 0.0099 - val_yA_loss: 0.0082 - val_yB_loss: 0.0017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 0.0092 - yA_loss: 0.0079 - yB_loss: 0.0013 - val_loss: 0.0087 - val_yA_loss: 0.0074 - val_yB_loss: 0.0013\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 0.0083 - yA_loss: 0.0071 - yB_loss: 0.0012 - val_loss: 0.0078 - val_yA_loss: 0.0066 - val_yB_loss: 0.0012\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 294us/sample - loss: 0.0074 - yA_loss: 0.0063 - yB_loss: 0.0011 - val_loss: 0.0069 - val_yA_loss: 0.0059 - val_yB_loss: 9.9644e-04\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 0.0067 - yA_loss: 0.0056 - yB_loss: 0.0011 - val_loss: 0.0061 - val_yA_loss: 0.0052 - val_yB_loss: 9.4579e-04\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 294us/sample - loss: 0.0058 - yA_loss: 0.0049 - yB_loss: 8.3641e-04 - val_loss: 0.0054 - val_yA_loss: 0.0047 - val_yB_loss: 7.4989e-04\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 0.0051 - yA_loss: 0.0044 - yB_loss: 7.7087e-04 - val_loss: 0.0048 - val_yA_loss: 0.0041 - val_yB_loss: 6.7040e-04\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 0.0045 - yA_loss: 0.0038 - yB_loss: 7.0365e-04 - val_loss: 0.0042 - val_yA_loss: 0.0036 - val_yB_loss: 5.7673e-04\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 0.0039 - yA_loss: 0.0034 - yB_loss: 5.7978e-04 - val_loss: 0.0037 - val_yA_loss: 0.0031 - val_yB_loss: 5.6487e-04\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 0.0034 - yA_loss: 0.0029 - yB_loss: 5.2077e-04 - val_loss: 0.0031 - val_yA_loss: 0.0027 - val_yB_loss: 4.3957e-04\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 300us/sample - loss: 0.0031 - yA_loss: 0.0025 - yB_loss: 5.4693e-04 - val_loss: 0.0027 - val_yA_loss: 0.0023 - val_yB_loss: 3.7548e-04\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 0.0027 - yA_loss: 0.0022 - yB_loss: 4.6519e-04 - val_loss: 0.0024 - val_yA_loss: 0.0020 - val_yB_loss: 3.5081e-04\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 0.0022 - yA_loss: 0.0019 - yB_loss: 3.7597e-04 - val_loss: 0.0020 - val_yA_loss: 0.0017 - val_yB_loss: 2.9763e-04\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 0.0019 - yA_loss: 0.0016 - yB_loss: 2.9438e-04 - val_loss: 0.0017 - val_yA_loss: 0.0015 - val_yB_loss: 2.4881e-04\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 0.0016 - yA_loss: 0.0014 - yB_loss: 2.6110e-04 - val_loss: 0.0015 - val_yA_loss: 0.0013 - val_yB_loss: 2.3765e-04\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 0.0014 - yA_loss: 0.0011 - yB_loss: 2.0706e-04 - val_loss: 0.0012 - val_yA_loss: 0.0011 - val_yB_loss: 1.8267e-04\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 300us/sample - loss: 0.0012 - yA_loss: 9.5717e-04 - yB_loss: 2.0031e-04 - val_loss: 0.0010 - val_yA_loss: 8.7268e-04 - val_yB_loss: 1.4023e-04\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 9.3989e-04 - yA_loss: 7.9733e-04 - yB_loss: 1.4256e-04 - val_loss: 8.5804e-04 - val_yA_loss: 7.0896e-04 - val_yB_loss: 1.4908e-04\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 7.7854e-04 - yA_loss: 6.5729e-04 - yB_loss: 1.2125e-04 - val_loss: 6.9204e-04 - val_yA_loss: 5.9553e-04 - val_yB_loss: 9.6508e-05\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 6.8197e-04 - yA_loss: 5.3839e-04 - yB_loss: 1.4358e-04 - val_loss: 5.6667e-04 - val_yA_loss: 4.8505e-04 - val_yB_loss: 8.1622e-05\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 5.2669e-04 - yA_loss: 4.3946e-04 - yB_loss: 8.7234e-05 - val_loss: 4.7474e-04 - val_yA_loss: 3.8462e-04 - val_yB_loss: 9.0115e-05\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 301us/sample - loss: 4.2037e-04 - yA_loss: 3.5543e-04 - yB_loss: 6.4933e-05 - val_loss: 3.6629e-04 - val_yA_loss: 3.1289e-04 - val_yB_loss: 5.3398e-05\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 3.3583e-04 - yA_loss: 2.8210e-04 - yB_loss: 5.3731e-05 - val_loss: 3.2161e-04 - val_yA_loss: 2.4308e-04 - val_yB_loss: 7.8531e-05\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 2.6497e-04 - yA_loss: 2.2260e-04 - yB_loss: 4.2367e-05 - val_loss: 2.2902e-04 - val_yA_loss: 1.9669e-04 - val_yB_loss: 3.2329e-05\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 2.0993e-04 - yA_loss: 1.7488e-04 - yB_loss: 3.5050e-05 - val_loss: 1.8178e-04 - val_yA_loss: 1.5329e-04 - val_yB_loss: 2.8493e-05\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 306us/sample - loss: 1.5965e-04 - yA_loss: 1.3454e-04 - yB_loss: 2.5101e-05 - val_loss: 1.4552e-04 - val_yA_loss: 1.1976e-04 - val_yB_loss: 2.5755e-05\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 1.4272e-04 - yA_loss: 1.0327e-04 - yB_loss: 3.9453e-05 - val_loss: 1.0434e-04 - val_yA_loss: 8.7733e-05 - val_yB_loss: 1.6606e-05\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 9.7627e-05 - yA_loss: 7.7996e-05 - yB_loss: 1.9632e-05 - val_loss: 7.9203e-05 - val_yA_loss: 6.7418e-05 - val_yB_loss: 1.1785e-05\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 6.9675e-05 - yA_loss: 5.8025e-05 - yB_loss: 1.1650e-05 - val_loss: 5.8055e-05 - val_yA_loss: 4.9188e-05 - val_yB_loss: 8.8672e-06\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 5.0574e-05 - yA_loss: 4.2713e-05 - yB_loss: 7.8611e-06 - val_loss: 4.1894e-05 - val_yA_loss: 3.6195e-05 - val_yB_loss: 5.6993e-06\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 3.7563e-05 - yA_loss: 3.0895e-05 - yB_loss: 6.6676e-06 - val_loss: 3.6107e-05 - val_yA_loss: 2.5036e-05 - val_yB_loss: 1.1071e-05\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 2.7118e-05 - yA_loss: 2.1912e-05 - yB_loss: 5.2057e-06 - val_loss: 2.2722e-05 - val_yA_loss: 1.7907e-05 - val_yB_loss: 4.8156e-06\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 1.8867e-05 - yA_loss: 1.5396e-05 - yB_loss: 3.4711e-06 - val_loss: 1.5165e-05 - val_yA_loss: 1.2342e-05 - val_yB_loss: 2.8228e-06\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 1.3252e-05 - yA_loss: 1.0533e-05 - yB_loss: 2.7192e-06 - val_loss: 1.0288e-05 - val_yA_loss: 8.5720e-06 - val_yB_loss: 1.7159e-06\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 294us/sample - loss: 8.9957e-06 - yA_loss: 7.1515e-06 - yB_loss: 1.8442e-06 - val_loss: 7.1624e-06 - val_yA_loss: 5.6131e-06 - val_yB_loss: 1.5493e-06\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 7.8299e-06 - yA_loss: 4.6683e-06 - yB_loss: 3.1616e-06 - val_loss: 5.0555e-06 - val_yA_loss: 3.7826e-06 - val_yB_loss: 1.2730e-06\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 314us/sample - loss: 3.8386e-06 - yA_loss: 3.0238e-06 - yB_loss: 8.1482e-07 - val_loss: 2.8374e-06 - val_yA_loss: 2.3890e-06 - val_yB_loss: 4.4844e-07\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 309us/sample - loss: 3.1692e-06 - yA_loss: 1.9360e-06 - yB_loss: 1.2332e-06 - val_loss: 2.3977e-06 - val_yA_loss: 1.5577e-06 - val_yB_loss: 8.4004e-07\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 2.8927e-06 - yA_loss: 1.1797e-06 - yB_loss: 1.7130e-06 - val_loss: 2.0510e-06 - val_yA_loss: 8.0753e-07 - val_yB_loss: 1.2434e-06\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 1.1773e-06 - yA_loss: 7.1859e-07 - yB_loss: 4.5870e-07 - val_loss: 9.6990e-07 - val_yA_loss: 5.2485e-07 - val_yB_loss: 4.4504e-07\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 309us/sample - loss: 6.6369e-07 - yA_loss: 4.4012e-07 - yB_loss: 2.2357e-07 - val_loss: 3.8893e-07 - val_yA_loss: 3.2899e-07 - val_yB_loss: 5.9934e-08\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 309us/sample - loss: 5.6373e-07 - yA_loss: 2.5499e-07 - yB_loss: 3.0874e-07 - val_loss: 2.7782e-07 - val_yA_loss: 1.9315e-07 - val_yB_loss: 8.4668e-08\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 2.0469e-07 - yA_loss: 1.3966e-07 - yB_loss: 6.5028e-08 - val_loss: 1.2148e-07 - val_yA_loss: 1.0023e-07 - val_yB_loss: 2.1252e-08\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 1.3020e-07 - yA_loss: 7.5189e-08 - yB_loss: 5.5014e-08 - val_loss: 6.8499e-08 - val_yA_loss: 5.1140e-08 - val_yB_loss: 1.7359e-08\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 5.7369e-08 - yA_loss: 3.9995e-08 - yB_loss: 1.7374e-08 - val_loss: 4.6458e-08 - val_yA_loss: 2.9373e-08 - val_yB_loss: 1.7085e-08\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 4.0490e-07 - yA_loss: 2.1154e-08 - yB_loss: 3.8374e-07 - val_loss: 1.4435e-07 - val_yA_loss: 1.2425e-08 - val_yB_loss: 1.3193e-07\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 1.3686e-07 - yA_loss: 1.0715e-08 - yB_loss: 1.2614e-07 - val_loss: 8.8085e-09 - val_yA_loss: 6.8965e-09 - val_yB_loss: 1.9121e-09\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 2.1804e-08 - yA_loss: 4.8082e-09 - yB_loss: 1.6995e-08 - val_loss: 1.7308e-07 - val_yA_loss: 4.9676e-09 - val_yB_loss: 1.6811e-07\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 6.4442e-07 - yA_loss: 2.8704e-09 - yB_loss: 6.4155e-07 - val_loss: 9.1812e-07 - val_yA_loss: 1.7081e-10 - val_yB_loss: 9.1795e-07\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 3.9699e-06 - yA_loss: 7.2122e-09 - yB_loss: 3.9626e-06 - val_loss: 1.3517e-06 - val_yA_loss: 4.0653e-09 - val_yB_loss: 1.3477e-06\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 322us/sample - loss: 1.4561e-05 - yA_loss: 3.7719e-08 - yB_loss: 1.4523e-05 - val_loss: 1.1941e-05 - val_yA_loss: 2.0230e-08 - val_yB_loss: 1.1921e-05\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 301us/sample - loss: 4.7473e-04 - yA_loss: 6.3943e-07 - yB_loss: 4.7409e-04 - val_loss: 6.8771e-06 - val_yA_loss: 3.0648e-08 - val_yB_loss: 6.8464e-06\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 3.0055e-06 - yA_loss: 5.3086e-09 - yB_loss: 3.0002e-06 - val_loss: 5.5189e-07 - val_yA_loss: 2.4443e-09 - val_yB_loss: 5.4945e-07\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 5.4782e-08 - yA_loss: 4.1423e-10 - yB_loss: 5.4368e-08 - val_loss: 5.1941e-09 - val_yA_loss: 1.3675e-10 - val_yB_loss: 5.0573e-09\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 320us/sample - loss: 3.5646e-09 - yA_loss: 1.7026e-10 - yB_loss: 3.3943e-09 - val_loss: 2.4587e-09 - val_yA_loss: 8.0113e-11 - val_yB_loss: 2.3786e-09\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 3.8971e-10 - yA_loss: 9.0798e-11 - yB_loss: 2.9891e-10 - val_loss: 3.6513e-10 - val_yA_loss: 8.3837e-11 - val_yB_loss: 2.8130e-10\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 1.0708e-10 - yA_loss: 5.6895e-11 - yB_loss: 5.0186e-11 - val_loss: 5.0843e-11 - val_yA_loss: 4.1253e-11 - val_yB_loss: 9.5899e-12\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 300us/sample - loss: 7.2313e-11 - yA_loss: 3.6656e-11 - yB_loss: 3.5657e-11 - val_loss: 3.5055e-11 - val_yA_loss: 2.8589e-11 - val_yB_loss: 6.4667e-12\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 1.6759e-10 - yA_loss: 2.3966e-11 - yB_loss: 1.4363e-10 - val_loss: 3.8548e-10 - val_yA_loss: 2.3757e-11 - val_yB_loss: 3.6172e-10\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 4.7058e-11 - yA_loss: 1.4940e-11 - yB_loss: 3.2119e-11 - val_loss: 2.0623e-11 - val_yA_loss: 1.2932e-11 - val_yB_loss: 7.6910e-12\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 2.3015e-11 - yA_loss: 1.1510e-11 - yB_loss: 1.1505e-11 - val_loss: 1.9466e-11 - val_yA_loss: 1.0939e-11 - val_yB_loss: 8.5267e-12\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 7.0070e-10 - yA_loss: 1.0221e-11 - yB_loss: 6.9048e-10 - val_loss: 4.9732e-11 - val_yA_loss: 8.3382e-12 - val_yB_loss: 4.1393e-11\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 333us/sample - loss: 1.7767e-09 - yA_loss: 1.0129e-11 - yB_loss: 1.7665e-09 - val_loss: 3.0983e-10 - val_yA_loss: 5.4802e-12 - val_yB_loss: 3.0435e-10\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 309us/sample - loss: 4.8851e-09 - yA_loss: 7.5021e-12 - yB_loss: 4.8776e-09 - val_loss: 5.7672e-08 - val_yA_loss: 1.0102e-10 - val_yB_loss: 5.7571e-08\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 4.4086e-08 - yA_loss: 8.1065e-11 - yB_loss: 4.4005e-08 - val_loss: 7.0033e-10 - val_yA_loss: 1.6445e-11 - val_yB_loss: 6.8389e-10\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 8.5203e-07 - yA_loss: 1.3498e-09 - yB_loss: 8.5068e-07 - val_loss: 4.1049e-06 - val_yA_loss: 5.4830e-09 - val_yB_loss: 4.0994e-06\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 353us/sample - loss: 9.1929e-07 - yA_loss: 1.6284e-09 - yB_loss: 9.1766e-07 - val_loss: 1.1134e-09 - val_yA_loss: 6.2276e-11 - val_yB_loss: 1.0511e-09\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 357us/sample - loss: 2.5528e-10 - yA_loss: 1.5693e-11 - yB_loss: 2.3958e-10 - val_loss: 2.0296e-11 - val_yA_loss: 6.0375e-12 - val_yB_loss: 1.4258e-11\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 2.3112e-11 - yA_loss: 4.8869e-12 - yB_loss: 1.8225e-11 - val_loss: 7.9394e-12 - val_yA_loss: 2.6054e-12 - val_yB_loss: 5.3341e-12\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 323us/sample - loss: 4.2552e-09 - yA_loss: 9.9362e-12 - yB_loss: 4.2453e-09 - val_loss: 2.6441e-10 - val_yA_loss: 1.7042e-14 - val_yB_loss: 2.6440e-10\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 327us/sample - loss: 5.9719e-09 - yA_loss: 1.3792e-11 - yB_loss: 5.9581e-09 - val_loss: 8.4318e-10 - val_yA_loss: 4.2496e-12 - val_yB_loss: 8.3893e-10\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 313us/sample - loss: 1.4227e-06 - yA_loss: 3.3158e-09 - yB_loss: 1.4194e-06 - val_loss: 2.2445e-05 - val_yA_loss: 3.1523e-08 - val_yB_loss: 2.2413e-05\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 8.1703e-05 - yA_loss: 2.1160e-07 - yB_loss: 8.1492e-05 - val_loss: 1.0327e-04 - val_yA_loss: 1.6084e-07 - val_yB_loss: 1.0311e-04\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 2.0368e-05 - yA_loss: 3.0607e-08 - yB_loss: 2.0338e-05 - val_loss: 1.1267e-05 - val_yA_loss: 1.7702e-08 - val_yB_loss: 1.1249e-05\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 1.1557e-05 - yA_loss: 2.4826e-08 - yB_loss: 1.1533e-05 - val_loss: 2.7924e-06 - val_yA_loss: 2.1311e-08 - val_yB_loss: 2.7711e-06\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 1.1346e-06 - yA_loss: 3.5566e-09 - yB_loss: 1.1310e-06 - val_loss: 8.1392e-09 - val_yA_loss: 2.1648e-10 - val_yB_loss: 7.9228e-09\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 343us/sample - loss: 1.4894e-06 - yA_loss: 2.7595e-09 - yB_loss: 1.4866e-06 - val_loss: 3.5453e-07 - val_yA_loss: 5.6377e-10 - val_yB_loss: 3.5397e-07\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 309us/sample - loss: 1.1986e-04 - yA_loss: 1.7713e-07 - yB_loss: 1.1968e-04 - val_loss: 6.7058e-04 - val_yA_loss: 1.0097e-06 - val_yB_loss: 6.6957e-04\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 338us/sample - loss: 1.3971e-04 - yA_loss: 2.1493e-07 - yB_loss: 1.3949e-04 - val_loss: 1.6926e-06 - val_yA_loss: 7.3157e-09 - val_yB_loss: 1.6852e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yA_weights\n",
      " [array([[1.9998486]], dtype=float32), array([0.9999806], dtype=float32)] \n",
      "\n",
      "yB_weights\n",
      " [array([[2.0001214],\n",
      "       [2.9998202]], dtype=float32), array([0.9995081], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "a = 2; b = 1; c = 2; d = 3; e = 1\n",
    "\n",
    "x_train_A = np.random.rand(1000,1) * 2 - 1\n",
    "x_train_B = np.random.rand(1000,1) * 2 - 1\n",
    "y_train_A = a * x_train_A + b\n",
    "y_train_B = c * np.exp(y_train_A) + d * x_train_B + e\n",
    "\n",
    "xA = layers.Input((1,), name='xA')\n",
    "xB = layers.Input((1,), name='xB')\n",
    "yA = layers.Dense(1, name='yA')(xA)\n",
    "h1 = layers.Lambda(lambda x: K.exp(x), name='exp')(yA)\n",
    "h2 = layers.Concatenate(name='concat')([h1, xB])\n",
    "yB = layers.Dense(1, name='yB')(h2)\n",
    "\n",
    "model = models.Model([xA, xB], [yA, yB])\n",
    "model.summary()\n",
    "\n",
    "model.compile('adam', 'mse')\n",
    "hist = model.fit([x_train_A, x_train_B], [y_train_A, y_train_B],\n",
    "                 batch_size=8, epochs=200, validation_split=0.2)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc=0)\n",
    "plt.show()\n",
    "\n",
    "yA_weights = model.get_layer('yA').get_weights()\n",
    "yB_weights = model.get_layer('yB').get_weights()\n",
    "print('yA_weights\\n', yA_weights, '\\n')\n",
    "print('yB_weights\\n', yB_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
