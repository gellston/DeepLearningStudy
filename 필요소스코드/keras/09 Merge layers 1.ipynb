{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda layer 와 Concatenate layer 를 사용하여 y = c0 + c1 * x + c2 * x^2 + c3 * x^3 관계를 가진 데이터에서 학습을 통하여 c0, c1, c2, c3 를 찾아내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h1_ABC (Lambda)                 [(None, 1), (None, 1 0           x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 3)            0           h1_ABC[0][0]                     \n",
      "                                                                 h1_ABC[0][1]                     \n",
      "                                                                 h1_ABC[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "y (Dense)                       (None, 1)            4           concat[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 1s 768us/sample - loss: 26078.5320 - val_loss: 27307.4287\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 233us/sample - loss: 24750.3560 - val_loss: 25904.2774\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 210us/sample - loss: 23460.2449 - val_loss: 24513.5854\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 216us/sample - loss: 22225.3991 - val_loss: 23218.1583\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 219us/sample - loss: 21057.1605 - val_loss: 21982.4864\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 248us/sample - loss: 19928.0748 - val_loss: 20816.3797\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 234us/sample - loss: 18848.8559 - val_loss: 19647.5122\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 223us/sample - loss: 17804.5539 - val_loss: 18561.7258\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 236us/sample - loss: 16805.7281 - val_loss: 17471.6770\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 233us/sample - loss: 15840.3807 - val_loss: 16493.2174\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 234us/sample - loss: 14925.1742 - val_loss: 15496.7819\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 219us/sample - loss: 14044.1112 - val_loss: 14580.6685\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 246us/sample - loss: 13204.0971 - val_loss: 13690.0755\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 223us/sample - loss: 12398.3893 - val_loss: 12845.4393\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 231us/sample - loss: 11628.8308 - val_loss: 12035.7496\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 246us/sample - loss: 10892.6434 - val_loss: 11272.8893\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 236us/sample - loss: 10192.9920 - val_loss: 10518.9627\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 229us/sample - loss: 9522.3127 - val_loss: 9821.9305\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 265us/sample - loss: 8888.2497 - val_loss: 9149.2969\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 196us/sample - loss: 8282.4760 - val_loss: 8521.6936\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 233us/sample - loss: 7709.9559 - val_loss: 7921.2294\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 227us/sample - loss: 7162.8686 - val_loss: 7342.6734\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 223us/sample - loss: 6642.4700 - val_loss: 6809.8028\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 268us/sample - loss: 6151.9161 - val_loss: 6288.9363\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 262us/sample - loss: 5686.1421 - val_loss: 5803.0587\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 211us/sample - loss: 5244.4126 - val_loss: 5353.9087\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 229us/sample - loss: 4831.2439 - val_loss: 4898.5877\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 217us/sample - loss: 4435.5390 - val_loss: 4509.8761\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 231us/sample - loss: 4068.9479 - val_loss: 4126.2938\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 233us/sample - loss: 3722.4512 - val_loss: 3764.5885\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 216us/sample - loss: 3396.3224 - val_loss: 3435.4789\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 256us/sample - loss: 3093.8928 - val_loss: 3123.0596\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 226us/sample - loss: 2812.6843 - val_loss: 2819.9242\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 228us/sample - loss: 2549.1976 - val_loss: 2547.2890\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 257us/sample - loss: 2303.9222 - val_loss: 2303.6418\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 218us/sample - loss: 2078.9361 - val_loss: 2064.1404\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 218us/sample - loss: 1868.5836 - val_loss: 1852.6280\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 229us/sample - loss: 1675.2400 - val_loss: 1660.9426\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 264us/sample - loss: 1498.3712 - val_loss: 1478.8885\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 250us/sample - loss: 1335.5932 - val_loss: 1312.2792\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 242us/sample - loss: 1187.4770 - val_loss: 1162.7711\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 231us/sample - loss: 1051.8427 - val_loss: 1025.5530\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 252us/sample - loss: 929.0237 - val_loss: 904.1145\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 212us/sample - loss: 818.4642 - val_loss: 791.6148\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 257us/sample - loss: 718.7628 - val_loss: 690.5920\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 213us/sample - loss: 629.1151 - val_loss: 603.5893\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 247us/sample - loss: 549.4145 - val_loss: 524.1543\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 218us/sample - loss: 478.2770 - val_loss: 455.9120\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 251us/sample - loss: 416.0106 - val_loss: 394.0695\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 236us/sample - loss: 360.4385 - val_loss: 340.6167\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 262us/sample - loss: 312.0437 - val_loss: 292.6304\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 257us/sample - loss: 270.0028 - val_loss: 250.9887\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 258us/sample - loss: 233.2592 - val_loss: 217.0276\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 227us/sample - loss: 201.5788 - val_loss: 187.4922\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 219us/sample - loss: 174.5788 - val_loss: 160.0969\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 280us/sample - loss: 150.8309 - val_loss: 138.8718\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 232us/sample - loss: 130.7944 - val_loss: 119.3810\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 242us/sample - loss: 113.5609 - val_loss: 103.5314\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 258us/sample - loss: 98.9332 - val_loss: 89.7627\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 222us/sample - loss: 86.3990 - val_loss: 78.0513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 261us/sample - loss: 75.7363 - val_loss: 68.1379\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 226us/sample - loss: 66.6367 - val_loss: 59.6322\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 254us/sample - loss: 58.7621 - val_loss: 52.5804\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 229us/sample - loss: 52.0597 - val_loss: 46.0981\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 218us/sample - loss: 46.2360 - val_loss: 40.8965\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 224us/sample - loss: 41.2344 - val_loss: 36.0376\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 274us/sample - loss: 36.8760 - val_loss: 32.0692\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 219us/sample - loss: 33.1072 - val_loss: 28.5923\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 231us/sample - loss: 29.8771 - val_loss: 25.6113\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 233us/sample - loss: 27.0295 - val_loss: 22.9807\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 251us/sample - loss: 24.5537 - val_loss: 20.7090\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 217us/sample - loss: 22.3994 - val_loss: 18.7634\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 236us/sample - loss: 20.5403 - val_loss: 17.0367\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 251us/sample - loss: 18.9398 - val_loss: 15.6179\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 226us/sample - loss: 17.5660 - val_loss: 14.3946\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 246us/sample - loss: 16.3870 - val_loss: 13.3767\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 247us/sample - loss: 15.3758 - val_loss: 12.5253\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 211us/sample - loss: 14.5295 - val_loss: 11.8097\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 0s 229us/sample - loss: 13.7980 - val_loss: 11.2107\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 242us/sample - loss: 13.1990 - val_loss: 10.7239\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 212us/sample - loss: 12.6762 - val_loss: 10.3204\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 236us/sample - loss: 12.2533 - val_loss: 9.9897\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 221us/sample - loss: 11.8764 - val_loss: 9.7225\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 228us/sample - loss: 11.5550 - val_loss: 9.4922\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 211us/sample - loss: 11.2853 - val_loss: 9.2934\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 228us/sample - loss: 11.0285 - val_loss: 9.1197\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 218us/sample - loss: 10.8045 - val_loss: 8.9628\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 231us/sample - loss: 10.5960 - val_loss: 8.8269\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 245us/sample - loss: 10.3849 - val_loss: 8.6791\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 221us/sample - loss: 10.1925 - val_loss: 8.5434\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 234us/sample - loss: 10.0118 - val_loss: 8.4013\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 212us/sample - loss: 9.8181 - val_loss: 8.2550\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 212us/sample - loss: 9.6197 - val_loss: 8.1200\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 214us/sample - loss: 9.4302 - val_loss: 7.9821\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 218us/sample - loss: 9.2398 - val_loss: 7.8319\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 242us/sample - loss: 9.0340 - val_loss: 7.6754\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 251us/sample - loss: 8.8315 - val_loss: 7.5094\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 223us/sample - loss: 8.6300 - val_loss: 7.3454\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 229us/sample - loss: 8.4289 - val_loss: 7.1792\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 212us/sample - loss: 8.2150 - val_loss: 6.9876\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 217us/sample - loss: 7.9991 - val_loss: 6.8105\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 229us/sample - loss: 7.7784 - val_loss: 6.6511\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 216us/sample - loss: 7.5612 - val_loss: 6.4832\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 226us/sample - loss: 7.3548 - val_loss: 6.3078\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 258us/sample - loss: 7.1193 - val_loss: 6.0955\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 221us/sample - loss: 6.8944 - val_loss: 5.9094\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 236us/sample - loss: 6.6649 - val_loss: 5.7287\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 219us/sample - loss: 6.4346 - val_loss: 5.5256\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 236us/sample - loss: 6.2150 - val_loss: 5.3291\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 229us/sample - loss: 5.9748 - val_loss: 5.1402\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 229us/sample - loss: 5.7553 - val_loss: 4.9451\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 235us/sample - loss: 5.5258 - val_loss: 4.7584\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 219us/sample - loss: 5.2972 - val_loss: 4.5607\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 212us/sample - loss: 5.0756 - val_loss: 4.3937\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 224us/sample - loss: 4.8544 - val_loss: 4.1736\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 243us/sample - loss: 4.6319 - val_loss: 4.0380\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 237us/sample - loss: 4.4072 - val_loss: 3.8086\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 263us/sample - loss: 4.2068 - val_loss: 3.6368\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 249us/sample - loss: 3.9974 - val_loss: 3.4843\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 243us/sample - loss: 3.7766 - val_loss: 3.2762\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 226us/sample - loss: 3.5768 - val_loss: 3.0992\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 229us/sample - loss: 3.3863 - val_loss: 2.9230\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 223us/sample - loss: 3.1914 - val_loss: 2.7611\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 232us/sample - loss: 3.0038 - val_loss: 2.6369\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 232us/sample - loss: 2.8167 - val_loss: 2.4464\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 216us/sample - loss: 2.6400 - val_loss: 2.2992\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 214us/sample - loss: 2.4657 - val_loss: 2.1440\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 223us/sample - loss: 2.2959 - val_loss: 2.0411\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 218us/sample - loss: 2.1446 - val_loss: 1.8691\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 232us/sample - loss: 1.9912 - val_loss: 1.7323\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 233us/sample - loss: 1.8436 - val_loss: 1.6222\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 232us/sample - loss: 1.7009 - val_loss: 1.4855\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 224us/sample - loss: 1.5641 - val_loss: 1.3859\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 221us/sample - loss: 1.4398 - val_loss: 1.2580\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 232us/sample - loss: 1.3206 - val_loss: 1.1711\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 223us/sample - loss: 1.2068 - val_loss: 1.0553\n",
      "Epoch 137/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 216us/sample - loss: 1.0899 - val_loss: 0.9653\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 222us/sample - loss: 0.9985 - val_loss: 0.8908\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 209us/sample - loss: 0.9006 - val_loss: 0.7879\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 204us/sample - loss: 0.8122 - val_loss: 0.7087\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 218us/sample - loss: 0.7323 - val_loss: 0.6431\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 209us/sample - loss: 0.6477 - val_loss: 0.5825\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 207us/sample - loss: 0.5775 - val_loss: 0.5110\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 218us/sample - loss: 0.5122 - val_loss: 0.4524\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 212us/sample - loss: 0.4527 - val_loss: 0.3993\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 208us/sample - loss: 0.4000 - val_loss: 0.3486\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 227us/sample - loss: 0.3436 - val_loss: 0.3114\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 221us/sample - loss: 0.3043 - val_loss: 0.2670\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 206us/sample - loss: 0.2610 - val_loss: 0.2289\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 223us/sample - loss: 0.2236 - val_loss: 0.1998\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 209us/sample - loss: 0.1910 - val_loss: 0.1741\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 207us/sample - loss: 0.1628 - val_loss: 0.1418\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 213us/sample - loss: 0.1358 - val_loss: 0.1223\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 206us/sample - loss: 0.1134 - val_loss: 0.0995\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 203us/sample - loss: 0.0944 - val_loss: 0.0824\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 224us/sample - loss: 0.0772 - val_loss: 0.0684\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 223us/sample - loss: 0.0637 - val_loss: 0.0554\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 217us/sample - loss: 0.0511 - val_loss: 0.0445\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 228us/sample - loss: 0.0410 - val_loss: 0.0357\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 231us/sample - loss: 0.0324 - val_loss: 0.0280\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 0s 217us/sample - loss: 0.0253 - val_loss: 0.0217\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 217us/sample - loss: 0.0196 - val_loss: 0.0175\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 211us/sample - loss: 0.0148 - val_loss: 0.0128\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 0s 208us/sample - loss: 0.0114 - val_loss: 0.0099\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 211us/sample - loss: 0.0085 - val_loss: 0.0072\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 222us/sample - loss: 0.0061 - val_loss: 0.0053\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 226us/sample - loss: 0.0045 - val_loss: 0.0038\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 268us/sample - loss: 0.0031 - val_loss: 0.0027\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 261us/sample - loss: 0.0022 - val_loss: 0.0018\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 227us/sample - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 226us/sample - loss: 0.0010 - val_loss: 8.4857e-04\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 222us/sample - loss: 6.7662e-04 - val_loss: 5.7087e-04\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 216us/sample - loss: 4.4212e-04 - val_loss: 3.5596e-04\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 216us/sample - loss: 2.7663e-04 - val_loss: 2.2740e-04\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 228us/sample - loss: 1.7035e-04 - val_loss: 1.5000e-04\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 228us/sample - loss: 1.0246e-04 - val_loss: 8.0428e-05\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 233us/sample - loss: 6.0681e-05 - val_loss: 4.6094e-05\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 231us/sample - loss: 3.4354e-05 - val_loss: 2.5983e-05\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 209us/sample - loss: 1.9322e-05 - val_loss: 1.5290e-05\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 224us/sample - loss: 1.0345e-05 - val_loss: 7.5080e-06\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 227us/sample - loss: 5.2278e-06 - val_loss: 3.8712e-06\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 233us/sample - loss: 2.6948e-06 - val_loss: 1.9132e-06\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 224us/sample - loss: 1.2823e-06 - val_loss: 9.2665e-07\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 242us/sample - loss: 6.1561e-07 - val_loss: 4.0622e-07\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 237us/sample - loss: 2.6712e-07 - val_loss: 1.9080e-07\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 238us/sample - loss: 1.1959e-07 - val_loss: 7.5024e-08\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 261us/sample - loss: 4.7456e-08 - val_loss: 3.0416e-08\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 222us/sample - loss: 1.9446e-08 - val_loss: 1.1731e-08\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 266us/sample - loss: 7.4393e-09 - val_loss: 4.5853e-09\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 229us/sample - loss: 2.7690e-09 - val_loss: 1.7191e-09\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 227us/sample - loss: 1.1378e-09 - val_loss: 7.6658e-10\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 239us/sample - loss: 5.3244e-10 - val_loss: 4.0262e-10\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 236us/sample - loss: 3.7052e-10 - val_loss: 4.4428e-10\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 251us/sample - loss: 3.4728e-10 - val_loss: 4.6587e-10\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 254us/sample - loss: 2.9088e-10 - val_loss: 3.2908e-10\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 228us/sample - loss: 3.1427e-10 - val_loss: 4.0658e-10\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 254us/sample - loss: 4.0799e-10 - val_loss: 9.6731e-10\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 224us/sample - loss: 3.3667e-10 - val_loss: 2.5749e-10\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 252us/sample - loss: 2.6677e-10 - val_loss: 2.8467e-10\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 254us/sample - loss: 3.4546e-10 - val_loss: 7.9432e-10\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_weights\n",
      "[array([[1.0000002],\n",
      "       [1.9999999],\n",
      "       [3.0000005]], dtype=float32), array([1.0000006], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "c0 = 1; c1 = 1; c2 = 2; c3 = 3\n",
    "\n",
    "x_train = np.random.rand(1000,1) * 10 - 5\n",
    "y_train = c0 + c1 * x_train +  c2 * x_train**2 + c3 * x_train**3\n",
    "\n",
    "x = layers.Input(shape=(1,), name='x')\n",
    "\n",
    "h1_A, h1_B, h1_C = layers.Lambda(lambda x: [x, x**2, x**3], name='h1_ABC')(x)\n",
    "h2 = layers.Concatenate(name='concat')([h1_A, h1_B, h1_C])\n",
    "y = layers.Dense(1, name='y')(h2)\n",
    "\n",
    "model = models.Model(x, y)\n",
    "model.summary()\n",
    "\n",
    "model.compile('adam', 'mse')\n",
    "hist = model.fit(x_train, y_train, batch_size=8, epochs=200, validation_split=0.2)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc=0)\n",
    "plt.show()\n",
    "\n",
    "y_weights = model.get_layer('y').get_weights()\n",
    "print('y_weights'); print(y_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
